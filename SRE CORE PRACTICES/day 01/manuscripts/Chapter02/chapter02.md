# Chapter 2 â€“ **â€œThe Problem Isnâ€™t Always the Problemâ€**

---

## Chapter Overview  
Itâ€™s 04 : 26 on a Tuesdayâ€”end-of-month settlement day at Kenya-Metro Bank. Overnight wire transfers that normally glide through the **swift-transfer-service** have begun to vanish into limbo. Corporate treasurers from Nairobi and Mombasa jab the red â€œpriority supportâ€ hotline, yet the main Geneos dashboard glows comfortingly green. CPU, memory, and even network utilisation hover in the low forties; Ops shift-lead Manu Gitonga would swear everything is fineâ€”if only Hectorâ€™s pager werenâ€™t lighting up like Diwali.  

Moments earlier, a junior SRE pushed a *seemingly* harmless Helm values patch during a routine one-node drain: `enableTracing=false`â€”meant for dev clustersâ€”slipped unnoticed into production. Now every outbound wire request sails through the code path **without** emitting a single span. Logs record success messages, but sender accounts never debit, receivers never credit, and the regulatorâ€™s resiliency clock is already ticking.  

This chapter deep-dives into a subtler observability failure: when instrumentation itself breaks and the team chases the **wrong evidence**. Youâ€™ll watch Hector drag the crew from seductive CPU graphs to the real culpritâ€”disabled tracing headersâ€”and learn the defensive patterns that keep telemetry from switching itself off.  

By the curtain call, you should recognise that â€œthe metric looks fineâ€ is a statement about your *tools*, not your *system*, and that every deployment pipeline needs a guardrail that screams if telemetry is missing before users do.

---

### ğŸ¯ Learning Objective  
Illustrate how mis-configured instrumentation can erase critical evidence, delaying diagnosis, and teach teams to harden tracing defaults, lock log schemas, and enforce post-deployment observability checks.

### âœ… Takeaway  
If your code can run without leaving breadcrumbs, it willâ€”never ship a build that can hide its own crimes.

### ğŸš¦ Applied Example  
At 04 : 26 the **swift-transfer-service** begins accepting 8 784 wire-transfer API calls per minute but writing **zero** rows to the settlement ledger. Legacy Geneos shows *Transport Thread CPU* at a placid 42 %, so Ops ignores the first call-centre escalation. A back-office recon script finally flags a KSh 1.7 billion â€œin-flightâ€ delta.  

A quick `grep "WireTransferRequest"` on the service logs reveals only `INFO WireTransferRequest received id=â€¦`â€”no `trace_id`, no `span_id`, and, most damningly, no `transaction_id`. A follow-up Jaeger query (`service=swift-transfer-service operation=WireTransferProcess`) returns **0 spans** for the last hour. Root cause: a Helm release that set `enableTracing=false`, propagated via ConfigMap reload without triggering a pipeline test. With traces gone, the team wasted 42 minutes inspecting CPU, GC, and NAT gateway metrics before discovering the silent mis-configuration. (â‰ˆ 160 words)

---

### Panel 1 â€“ *False Green, True Red*  
A frantic call-centre agent waves a yellow â€œP1â€ escalation sheet as Manu studies a serene CPU time-series on the wall-mounted Geneos board.

> **Agent (voice over speaker):** â€œCorporate wires are hangingâ€”Treasury says *nothing* is posting!â€  
> **Manu:** â€œCPU is forty-two percent; nothingâ€™s choking.â€  
> **Hector** *(bursting through the door, half-tied shoelaces)*: â€œWhen you stare at the wrong metric, every problem looks like not-a-problem.â€  
> **Wanjiru:** â€œWhat should we check first?â€  
> **Hector:** â€œWhatever the customer feels *before* we feel a graphâ€”throughput of posted wires.â€

Hector flips to Grafana, types a quick PromQL:  

```promql
sum(rate(wire_posted_total[1m]))
```  

The stat card plummets to **0.0 wires/s**.

![Panel 1 â€“ False green dashboard](images/ch2_p1_mystery.png){width=600}

:::hector-aphorism  
â€œYou canâ€™t KPI your way out of an outage you canâ€™t see.â€  
:::

---

### Panel 2 â€“ *The Seductive Dashboard*  
The Geneos â€œGlobal Healthâ€ dial still points blissfully at **GOOD**; Manu refreshes, hoping red will appear.

> **Manu:** â€œStill no spikesâ€”maybe queue depth?â€  
> **Hector:** â€œMaybe weâ€™re measuring *the wrong queue*. Metrics that donâ€™t map to money are vanity.â€  
> **Wanjiru:** â€œLedger-write latency?â€  
> **Hector:** â€œGood instinctâ€”split by transfer type and region.â€

Wanjiruâ€™s ad-hoc query shows ledger latency flatlining at 10 msâ€”too perfect. Hector squints: â€œIf the service never writes, latency looks perfect.â€ A haunting realisation dawns.

![Panel 2 â€“ Seductive dashboard](images/ch2_p2_dashboard.png){width=600}

---

### Panel 3 â€“ *The Case of the Missing Spans*  
Juana tails the service logs; each record ends with `trace_id=null`. She opens Jaegerâ€”empty results. No spans, no service map, no dependency diagram.

```shell
$ jwq 'service("swift-transfer-service") | last 1h'
# 0 traces
```

> **Juana:** â€œJaegerâ€™s got nothingâ€”tracing header is blank!â€  
> **Hector:** â€œObservability budgets are like oxygen: if someone toggles them off, you suffocate before you notice.â€  
> **Manu:** â€œWho flipped the switch?â€  
> **Hector:** â€œGit history will tell usâ€”but first, prove that missing traces caused the blind spot.â€

Juana reviews the last Helm diff:  
```diff
-enableTracing: true
+enableTracing: false
```

![Panel 3 â€“ Missing spans](images/ch2_p3_no_trace.png){width=600}

:::system-failure-anecdote  
*Last quarter, a junior dev removed request logging â€œfor privacyâ€ in the AML service. Fraud spikes went undetected for 36 hours until regulators called. The fix wasnâ€™t more dashboards; it was a pipeline rule: **log schema must never shrink** without an SRE sign-off.*  
:::

---

### Panel 4 â€“ *Flashback: The Silent Toggle*  
A security-cam-style freeze-frame appears on the NOC monitor: six hours earlier in Dev Lab-3. Junior engineer **Bilal** merges PR #8421 while Wanjiru rubber-stamps the review.

```diff
- enableTracing: true   # default
+ enableTracing: false  # faster tests
```

> **Bilal:** â€œJust mirroring dev valuesâ€”no functional change.â€  
> **Wanjiru (yesterday):** â€œLooks harmless; approved.â€  
> **Hector (now, pointing at the diff):** â€œOne line. One silent oxygen cutoff.â€  
> **Wanjiru (present-day, mortified):** â€œI missed the ConfigMap pathâ€”thought it was for the canary cluster.â€  
> **Hector:** â€œPRs are history lessons written in blood. Letâ€™s learn before we bleed again.â€

![Panel 4 â€“ Flashback config push](images/ch2_p4_flashback.png){width=600}

---

### Panel 5 â€“ *Blame Game in the War-Room*  
Back to the NOC. Dev and Ops huddle around a high table stacked with laptops; voices overlap.

> **Ops-Lead (arms crossed):** â€œTracing flags are a *dev* concernâ€”you broke prod.â€  
> **Bilal (defensive):** â€œConfig passed CI; Ops promoted it!â€  
> **Hector (slamming a marker on table):** â€œTelemetry is *everyoneâ€™s* concern. If a toggle can erase evidence, the pipeline is guilty.â€  
> **Juana:** â€œRestore tracing, rerun Jaeger, then add a deploy-gate: *abort if spans = 0*.â€  
> **Manu (typing):** â€œRolling back ConfigMapâ€¦ tracing header live in 90 seconds.â€

A shared Grafana panel refreshes: span counts climb from **0** to **14 k/min**.

![Panel 5 â€“ Ops vs Dev finger-pointing](images/ch2_p5_blame.png){width=600}

---

### Panel 6 â€“ *Instrumentation as a First-Class Citizen*  
Hector steps to a smartboard, sketching the tracing pipeline from app pod to Jaeger storage. Arrows animate live spans as they flow.

> **Hector:** â€œTracing isnâ€™t optional plumbing; itâ€™s the black-box recorder.â€  
> **Wanjiru:** â€œSo the toggle sits between pod and collector?â€  
> **Hector:** â€œExactly. Break that link, and every metric becomes a rumour.â€

![Panel 6 â€“ Tracing pipeline diagram](images/ch2_p6_hector.png){width=600}

:::diagram  
```mermaid
flowchart LR  
  subgraph App_Pod  
    A1(Request) --> A2(OpenTelemetry SDK)  
  end  
  A2 -->|OTLP gRPC| C[Collector] -->|Batch| S[Jaeger Storage]  
  S --> G[Grafana Tempo]  
  C -. healthcheck .-> M[Deploy Gate]  
  M -. "abort if spans==0" .-> K[Kubernetes Rollout]  
  style M fill:#ffd2d2,stroke:#c00,stroke-width:2  
```  
:::  

:::try-this  
**Pipeline Guard Drill**  
1. Pick one production service.  
2. Run a canary deploy with `enableTracing=false`.  
3. Your rollout controller **must abort** within 60 s.  
4. If it doesnâ€™t, add a gate: _â€œReject deploy when exported spans < threshold.â€_  
Describe in one sentence how youâ€™ll implement that guardrail this sprint.  
:::  

---

### Panel 7 â€“ *Telemetry Restored, Truth Revealed*  
Grafana flickersâ€”dependency map repopulates: `swift-transfer-service â†’ ledger-write â†’ db-shard-04`. Span view highlights a 12 s write on shard 04; throughput card climbs.

> **Juana:** â€œSpans back onlineâ€”look, 12 s write latency on shard 04!â€  
> **Manu:** â€œQueue depth rising again; ledger rows committing.â€  
> **Hector:** â€œFunny how the problem shows its face once it canâ€™t hide.â€

![Panel 7 â€“ Corrected spans visible](images/ch2_p7_corrected.png){width=600}

---

### Panel 8 â€“ *Lesson Etched in Ink*  
Sunlight creeps through the NOC blinds. Team gathers beneath the now-honest dashboard.

> **Wanjiru:** â€œThe problem wasnâ€™t CPUâ€¦ it was our blindfold.â€  
> **Hector:** â€œAn invisible system will fail in invisible ways. Our job is to make it confess before customers complain.â€

![Panel 8 â€“ Team reflection](images/ch2_p8_realize.png){width=600}

:::reflection  
Recall a recent deployment you approved. Could that build have shipped with **zero traces** or **missing log fields**?  
â†’ Write one pipeline check youâ€™ll add this week to guarantee evidence is never optional.  
:::  

---
