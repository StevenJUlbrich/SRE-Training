# ðŸ’» Observability 101 â€“ Junior SRE Edition  
*Day 1: â€œHow to Start Understanding Systems Without Panickingâ€*

ðŸŽ™ Narrated by Hector Alvarez â€“ uptime mechanic, log whisperer, and reluctant mentor.

> **Hectorâ€™s Monologue:** â€œEverybody loves a blinking dashboardâ€”until it blinks and you donâ€™t know why. Letâ€™s fix that.â€

---

## ðŸŽ¯ Goal

Help junior SREs move from reactive ticket-clicking to proactive system investigation.  
Learn how to navigate telemetry data, understand the Three Pillars of Observability, and start forming mental models for investigating real-world issues.

---

## ðŸ§  Who This Is For

Youâ€™ve done some support work. Youâ€™ve seen alerts. You may have clicked through dashboards without knowing if you were supposed to *feel something*. You're now entering the SRE journeyâ€”and this is your first step toward diagnosing, debugging, and improving system reliability.

> **Hectorâ€™s Monologue:** â€œYou donâ€™t need to know everything. But you do need to know where to look when it breaks.â€

---

## ðŸ” What Is Observability?

> **â€œObservability is how well you can understand what's going on inside a system based on the information it gives you.â€**  
> â€” *SRE Translation Service, Junior Edition*

Itâ€™s not just about reacting to alertsâ€”itâ€™s about being able to **ask your system intelligent questions** when things go wrong, and actually *get answers* without guessing.

> **Hectorâ€™s Monologue:** â€œIf monitoring is like a security alarm, observability is the security camera footage. It tells you what happened, not just that it did.â€

---

## ðŸ§± The Three Pillars of Observability

Youâ€™ll see these everywhere. They are the building blocks of how we observe complex systems.

### ðŸ“œ Logs

- Text records of what your services are doing.
- Can include errors, events, debug info.
- Usually what you read when someone says â€œgo check Splunk.â€

**Example:**
```spl
2024-04-17 10:32:12 [ERROR] AuthService - Invalid token for user_id=4412
```

> ðŸ§  Logs are great for answering:  
> - What happened?  
> - Where did it happen?  
> - Was there an error?

> **Hectorâ€™s Monologue:** â€œLogs are your witnesses. They donâ€™t always speak clearlyâ€”but they remember everything.â€

---

### ðŸ“ˆ Metrics

- Numbers that are measured over time.
- These live in dashboards like Geneos or DataDog.
- Metrics tell you how things are **performing**.

**Example:**
```
CPU usage = 92%
Request latency (p95) = 1.2s
Error rate = 3.4%
```

> ðŸ§  Metrics help you:  
> - Detect spikes or drops  
> - Monitor trends  
> - Trigger alerts when thresholds are crossed

> **Hectorâ€™s Monologue:** â€œMetrics tell you somethingâ€™s weird. They donâ€™t tell you *why*. Thatâ€™s someone elseâ€™s jobâ€”or yours.â€

---

### ðŸ§µ Traces

- Like a GPS map of a user request traveling through your system.
- Shows how long each service took and where problems happened.
- Often found in APM tools like DataDog or OpenTelemetry dashboards.

**Example:**  
A user login request took:
- 30ms in frontend
- 90ms in auth
- 600ms in database (ðŸ’¥ bottleneck)

> ðŸ§  Traces help you:  
> - Understand **where** delays happen  
> - Follow requests across **microservices**  
> - See full end-to-end behavior of your system

> **Hectorâ€™s Monologue:** â€œTraces are like detective workâ€”but faster, and nobody slams the table.â€

---

## ðŸ”„ Monitoring vs Observability

Letâ€™s make this painfully clear:

| ðŸ”§ Monitoring           | ðŸ”¬ Observability                |
|------------------------|--------------------------------|
| Predefined alerts       | Asking new questions during incidents |
| â€œIf X > 80%, page meâ€   | â€œWhy is X > 80%, and what else broke?â€ |
| Mostly reactive         | Both reactive *and* investigative |

> ðŸ§  Monitoring tells you *somethingâ€™s wrong*.  
> Observability helps you figure out *why* itâ€™s wrong.

> **Hectorâ€™s Monologue:** â€œMonitoring is a doorbell. Observability is looking through the peephole.â€

---

## ðŸ”” Alerts â€“ Learning to Read the Screaming

Youâ€™ve seen alerts. Some are helpful. Most are chaos.

Hereâ€™s what youâ€™re looking for in a **good alert**:
- It points to a **real problem**, not just â€œCPU spiked for 3 seconds.â€
- It includes enough context to help you start investigating.
- It doesnâ€™t fire constantly for no reason (hello, alert fatigue).

**Example Alert:**
> "Payment API p95 latency > 2s for 5 minutes â€“ related service: `order-service`, region: `us-east-1`"

**Not helpful alert:**
> â€œDisk warning: /dev/sda1 at 85%â€

> **Hectorâ€™s Monologue:** â€œIf the alert doesnâ€™t tell you what to check, itâ€™s not an alertâ€”itâ€™s background noise.â€

---

## ðŸ§ª Quick Activity: Match the Tool to the Pillar

Which of these tools help with which observability data?

| Tool     | Logs | Metrics | Traces |
|----------|------|---------|--------|
| Geneos   | âŒ   | âœ…      | âŒ     |
| Splunk   | âœ…   | âœ… (via saved searches) | âŒ     |
| DataDog  | âœ…   | âœ…      | âœ…     |

> ðŸ§  Optional: Open each tool (if you have access) and find an example of each type of data.

---

## ðŸ§  Triage Mindset: What Do I Look at First?

When you get an alert, try following this flow:

1. **What is the alert telling me?**
2. **Is this affecting users?** (high latency, errors, etc.)
3. **Check the dashboard:** Look for patterns or spikes
4. **Check logs:** Are there any error bursts?
5. **Check traces (if available):** Where is time being spent?

> **Hectorâ€™s Monologue:** â€œYouâ€™re not hunting for bugs. Youâ€™re looking for *cause*. Donâ€™t stop at the first error that looks guilty.â€

---

## ðŸ§ª Exercises

### 1. Alert to Investigation
Pick a recent alert from your environment (Geneos, DataDog, etc).  
Ask:
- What kind of data is it based on?
- Whatâ€™s your first move to investigate?

### 2. Splunk Log Search (Guided)
Use a basic Splunk query to find errors:

```spl
index=prod_logs "error" OR "exception"
| stats count by service_name
```
Whatâ€™s the top offending service?

### 3. Dashboard Reading Practice
Find a dashboard in Geneos or DataDog:
- What metric is being shown?
- Whatâ€™s the time window?
- Is there anything unusual?

---

## âœ… What You Should Know Now

You should now:
- Understand the difference between monitoring and observability
- Know the three types of telemetry data: logs, metrics, traces
- Be able to identify these in Splunk, Geneos, and DataDog
- Start building a mental model for how alerts â†’ investigation â†’ resolution

> **Hectorâ€™s Monologue:** â€œYou canâ€™t fix what you canâ€™t see. And you canâ€™t learn if you never look. Letâ€™s change that.â€

---

## â­ï¸ Next Brick: Correlating Logs, Metrics, and Traces

> In the next session:  
> Weâ€™ll learn how to combine these signals to find **root cause** and not just guess based on vibes.

