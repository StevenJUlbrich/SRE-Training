<!-- Part A of Chapter 6 -->

# Chapter 6 â€“ Tools of the Trade  
---

### Chapter Overview  

Prometheus, Grafana, and Splunk form the telemetry tripod that props up modern banking reliability. In this chapter, Ava Kimani teams up with CTO **Malik Njoroge** to bootstrap a full metrics â†’ alerting pipeline for the Fund-Transfer service. You will install Prometheus with secure TLS scrapes, craft Grafana dashboards that tell latency stories at a glance, hunt hidden anomalies with Splunk SPL, and finally wire every golden signal into a single end-to-end flow. By the time Malik rolls the stack into production, even the boardroom TV will glow with real-time trust.

---

## ğŸ¯ Learning Objective  

Install and configure Prometheus, Grafana, and Splunk for a core-banking API; build a latency dashboard, create burn-rate alerts, and trace log outliers back to metrics.

## âœ… Takeaway  

Great tools amplify good practice; mis-instrumented metrics only automate confusion.

## ğŸš¦ Applied Example  

Ava exposes a **`fund_transfer_latency_seconds`** histogram at `/metrics`.  
Prometheus scrapes it over TLS (`https://fund-api.internal:443/metrics`).  
Grafana visualises the p99 latency sparkline.  
Splunk pulls the same histogram via PushGateway, joins it to queue depth logs, and alerts when p99 > 300 ms **AND** `queue_depth > 80 %`.  
A 14Ã— one-hour burn-rate rule pages the on-call SRE. Malik watches the badge flip amber, rolls back a feature flag, and the sparkline returns greenâ€”all before customers tweet.

---

## Teaching Narrative 1 â€“ *Prometheus: Your Metric Collector*  

Daniel unpacks a Prometheus tarball on a staging VM. Ava peers over his shoulder.

**Ava:** â€œBefore `./prometheus`, harden it.â€  
She points to the banking controls checklist:

* **TLS scrape endpoints**  
* **Basic-auth on `/metrics`**  
* **Static configs pinned to service-discovery tags**  
* **Scrape interval = 15 s** for critical APIs

They create **`prometheus.yml`**:

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 30s
scrape_configs:
  - job_name: fund-api
    scheme: https
    tls_config:
      ca_file: /etc/pki/ca.crt
    basic_auth:
      username: metrics
      password: ${METRICS_PASS}
    static_configs:
      - targets:
          - fund-api.internal:443
        labels:
          env: staging
```

Prometheus starts with `--web.enable-lifecycle` to allow hot-reloads. Ava hits:

```bash
curl -k -u metrics:$METRICS_PASS \
  https://fund-api.internal:443/metrics \
  | head -5
```

Histogram buckets print. Prometheus UI shows **`up{job="fund-api"}`** = 1.

> *Scene*: Malik signs an internal PKI cert; risk officer checks a TLS cipher list: TLS 1.2, ECDHE-RSA-AES256-GCM-SHA384â€”approved.

Ava adds a recording rule:

```yaml
- record: api_latency_p99
  expr: histogram_quantile(
          0.99,
          sum(rate(fund_transfer_latency_seconds_bucket[5m])) by (le))
```

Daniel glances at the green â€œ2 / 2 loaded successfullyâ€ toast. â€œFeels like magic,â€ he says. Ava smiles: â€œMagic you can measure.â€

![Alt text](images/ch06_p01_prom_install.png){width=600}

---

## Teaching Narrative 2 â€“ *Grafana: Storytelling Dashboards*  

Grafana spins up on `https://grafana.internal`. Ava logs in with SSO, creates a **Golden Signals** folder, and adds a **Time series** panel:

* Query: `api_latency_p99{env="staging"}`
* Legend: `p99 latency (ms)`
* Unit: `ms`
* Thresholds: 300 ms amber, 400 ms red

A **Gauge** panel below uses:

```
expr: 1000 * histogram_quantile(
         0.99,
         sum(rate(fund_transfer_latency_seconds_bucket[1m])) by (le))
```

They bind both panels to a **variable `$env`** so QA can use the same dashboard.

Ava drags a **Bar gauge** for **Traffic** (`rate(http_requests_total[1m])`), a **Stat** for **Error ratio** (`1 - success_ratio`), and a **Bar gauge** for **Saturation** (`queue_depth / queue_capacity * 100`).

**Swahili proverb moment**

:::proverb  
> â€œPicha worth elfu moja maneno.â€ â€” *One picture is worth a thousand words.*  
:::

Malikâ€™s eyes widen as the p99 line crosses amber during load test. â€œCEO will love this,â€ he says. Ava replies, â€œAnd will hate redâ€”so keep it green.â€

![Alt text](images/ch06_p02_grafana_dash.png){width=600}

---

## Teaching Narrative 3 â€“ *Splunk: Hunting the Unknowns*  

Ava points Daniel toward **Splunk HEC** (HTTP Event Collector). They configure a **token-secured** input:

```bash
curl -k https://splunk.internal:8088/services/collector \
  -H "Authorization: Splunk $SPLUNK_TOKEN" \
  -d '{"event":"latency_bucket","bucket":"0.3","count":5}'
```

Next, they ingest fund-transfer logs:

```
2025-05-20T14:12:08Z level=error code=QUEUE_TIMEOUT ...
```

In **Search & Reporting** they craft a correlation SPL:

```spl
index=metrics sourcetype=prom_histogram
| eval bucket_ms=bucket*1000
| stats sum(count) AS hits BY _time bucket_ms
| join _time 
  [ search index=logs sourcetype=fund_transfer error=true
    | eval latency_ms=duration*1000
    | bucket span=1h _time
    | stats count AS err BY _time ]
| eval error_ratio=err/hits
| timechart span=1h max(error_ratio)
```

The chart spikes when bucket `>500 ms` coincides with queue depth alerts.

Dialogue:

**Malik:** â€œSo Splunk links errors to latency tail.â€  
**Ava:** â€œAnd proves our SLI is your saviour.â€

They schedule a Splunk alert: **error_ratio > 0.01** triggers `sre-oncall`.

![Alt text](images/ch06_p03_splunk_query.png){width=600}

---

## Teaching Narrative 4 â€“ *Wiring Golden Signals End-to-End*  

Ava grabs a marker and draws a topology:

:::diagram  
```mermaid
sequenceDiagram
  autonumber
  participant App as Mobile App
  participant Exp as Exporter
  participant Prom as Prometheus
  participant Graf as Grafana
  participant AM as Alertmanager
  participant Slack as Slack Bot

  App->>Exp: /metrics
  Exp->>Prom: scrape HTTPS 15 s
  Prom->>Graf: query p99
  Prom->>AM: burn-rate rule
  AM-->>Slack: P1 Pager
```  
:::

**Daniel** pastes the exporter container next to the app in Kubernetes, forgetting labels. Ava catches him.

:::slap  
*Stop scraping without job labelsâ€”troubleshooting blind!*  
:::

They add:

```yaml
metadata:
  labels:
    app: fund-api
    job: fund-transfer
```

Grafana variable dropdown now lists the job; dashboards updated automatically.

> **Scene**: Prometheus `targets` page lights green checks. Alertmanager web-UI shows two receivers: `oncall` and `exec-channel`.

![Alt text](images/ch06_p04_signal_flow.png){width=600}

---
<!-- Part B of Chapter 6 -->

## Teaching Narrative 5 â€“ *Burn-Rate Alerts & Slack Bot*  *(â‰ˆ 1 250 words)*  

Ava drafts a **PrometheusRule** that pages when the 99.9 % latency budget burns at 14 Ã— in one hour:

```yaml
groups:
- name: burn-rate.rules
  rules:
  - alert: SLOBurnRateFast
    expr: (
      increase(slo_breaches_total[1h])
      / 3600
    )
      / (error_budget_seconds / (30*24*3600)) > 14
    for: 5m
    labels:
      severity: critical
      route: oncall
    annotations:
      summary: "Fund-transfer SLO burning fast (14Ã—)"
      runbook: https://runbooks.bank/slo-budgets
```

In **Alertmanager** they add a Slack receiver:

```yaml
receivers:
- name: oncall
  slack_configs:
  - channel: "#sre-oncall"
    username: "Prometheus"
    send_resolved: true
```

Daniel tests with a cURL:

```bash
curl -XPOST localhost:9090/-/reload
curl -XPOST localhost:9093/-/reload
curl -XPOST localhost:9093/api/v2/alerts \
     -H "Content-Type: application/json" \
     -d '[{"labels":{"alertname":"SLOBurnRateFast","severity":"critical"}}]'
```

Slack lights up:  
**ğŸ”¥  SLOBurnRateFast** â€“ Fund-transfer SLO burning fast (14Ã—).

He adds a **cron Lambda** that posts a daily budget summary:

```json
ğŸŸ¡  Error-budget 72 % â€¢ Burn-rate 0.8Ã— â€¢  Deploys OPEN
```

### Learner Prompt  

:::exercise  
**Task:**  
1. Copy the `burn-rate.rules` group into your Prometheus config.  
2. Point Alertmanager to a test Slack webhook and trigger a synthetic alert with `curl`.  
3. Post a screenshot of the Slack message.  
:::

![Alt text](images/ch06_p05_burn_alert.png){width=600}

---

## Teaching Narrative 6 â€“ *Dashboards for the Boardroom*  *(â‰ˆ 1 250 words)*  

Malik wants a lobby-screen loop that even auditors understand. Ava duplicates the â€œGolden Signalsâ€ dashboard and replaces technical legends with plain language:

| Panel | Query | Friendly Title |
|-------|-------|----------------|
| Time-series | `api_latency_p99` | **â€œMoney-move speed (p99)â€** |
| Gauge | `1-success_ratio` | **â€œFailed transfers %â€** |
| Bar | `rate(http_requests_total[1m])` | **â€œTransfers per secondâ€** |

Ava then adds a **Stat** panel that shows **Remaining Budget %**, colour-coded ğŸŸ¢ > 80 %, ğŸŸ¡ 50-80 %, ğŸ”´ < 50 %.  Grafanaâ€™s **playlist** mode rotates the dashboard every 10 s on the boardroom TV.

**Dialogue**

**Malik:** â€œMarketing needs shiny KPI bars.â€  
**Ava:** â€œShiny is fineâ€”just ensure colour changes when trust erodes.â€  

They export the dashboard JSON and commit it to `dashboards/boardroom.json` so GitOps keeps the loop consistent across offices.

![Alt text](images/ch06_p06_boardroom_tv.png){width=600}

---

## Teaching Narrative 7 â€“ *Securing the Tool Chain*  *(â‰ˆ 1 200 words)*  

Regulators audit every telemetry component touching PII.  Ava walks Daniel through a **three-layer defence**:

1. **TLS everywhere** â€“ Prometheus â†” targets, Grafana â†” browser, Splunk HEC.  
2. **RBAC** â€“  
   * **viewer**: read-only; boardroom TV runs this token.  
   * **editor**: create panels; must pass security training.  
   * **admin**: change datasources; only SRE.  
3. **Audit Logs** â€“ Loki tail of Grafana admin changes shipped to Splunk.

In Kubernetes they apply a **NetworkPolicy** that allows Prometheus scrapes only from the SRE namespace and block egress to the internet.

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: prometheus-only-sre
spec:
  podSelector:
    matchLabels:
      app: prometheus
  policyTypes: ["Egress"]
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: sre
```

Risk Officer Kamau signs off after validating cipher suites with `nmap --script ssl-enum-ciphers`.  The dashboard now shows a green **â€œCompliance: PASSâ€** badge fetched from Splunk audit queries.

![Alt text](images/ch06_p07_security.png){width=600}

---

## Teaching Narrative 8 â€“ *Malikâ€™s First Production Roll-out*  *(â‰ˆ 1 250 words)*  

Malik merges a Helm chart PR:

```bash
helm upgrade --install tele-stack charts/telemetry \
  --set env=prod,alerts.slackChannel=sre-oncall
```

Argo CD syncs; Prometheus picks up **prod** scrape configs; Grafana dashboards auto-import.

Minutes later the **Budget Remaining dial** flips from grey to ğŸŸ¢ 92 %.  Malik posts a selfie with the boardroom screen.

### Try This  

:::exercise  
1. Fork the `tele-stack` chart.  
2. Override `scrapeInterval` to 10 s, deploy to staging, and import the â€œGolden Signalsâ€ dashboard.  
3. Post the dashboardâ€™s p99 latency line showing real data.  
:::

![Alt text](images/ch06_p08_malik_rollout.png){width=600}

---

<!-- Part C of Chapter 6 -->

## Teaching Narrative 9 â€“ *Alertmanager Routing Rules*  *(â‰ˆ 1 150 words)*  

Ava opens **alertmanager.yml** and adds a routing tree:

```yaml
route:
  receiver: default
  group_by: ['job']
  routes:
    - matchers:
        - severity="critical"
      receiver: oncall
      continue: true
    - matchers:
        - severity="exec"
      receiver: exec-channel
receivers:
- name: oncall
  slack_configs:
  - channel: "#sre-oncall"
- name: exec-channel
  slack_configs:
  - channel: "#bank-exec"
```

**Dialogue**  

**Malik:** â€œExec channel gets only ğŸ”´?â€  
**Ava:** â€œExactlyâ€”noise erodes trust.â€  

They label alerts with `severity: exec` when burn-rate > 50 Ã— for 5 m. A test alert hits **#bank-exec**: _â€œLatency meltdownâ€”deploy freeze enforced.â€_  

![Alt text](images/ch06_p09_alertmanager_routing.png){width=600}

---

## Teaching Narrative 10 â€“ *Grafana OnCall & Escalations*  *(â‰ˆ 1 150 words)*  

Grafana OnCall now integrates with Alertmanager. Ava creates an **escalation chain**:

1. **Tier 1** â€“ SRE-oncall (SMS + app) 10 minutes.  
2. **Tier 2** â€“ Platform lead (voice) next 10.  
3. **Tier 3** â€“ CTO Malik, Risk Officer Kamau.

She drags a threshold slider to test; phones buzz across the room.

**Dialogue**  

**Ava:** â€œEscalationâ€™s about accountability, not blame.â€  
**Daniel:** â€œAnd sleep!â€  

![Alt text](images/ch06_p10_grafana_oncall.png){width=600}

---

## Teaching Narrative 11 â€“ *Splunk Correlation Dashboards*  *(â‰ˆ 1 100 words)*  

Ava builds a Splunk **glass-table**: top row metrics, mid row logs, bottom row traces. Clicking the 500 ms p99 tile filters logs for `duration>0.5`.

Learner sees red nodes glowing along the trace waterfall; queue depth matches latency spikes.

No widget needed; includes dialogue:

**Learner:** â€œTraces tell the â€˜whyâ€™ behind the metric.â€  
**Ava:** â€œCorrelation turns mystery into math.â€  

![Alt text](images/ch06_p11_splunk_correlation.png){width=600}

---

## Teaching Narrative 12 â€“ *Cost & Capacity Dashboards*  *(â‰ˆ 1 100 words)*  

Finance asks: â€œHow much does telemetry cost?â€  Ava adds PromQL for TSDB chunks and Grafana **Pie chart**:

```
sum(prometheus_tsdb_head_chunks{instance=~".*prom.*"})
```

Alerts trigger when chunks > 6 M (storage blow-up). A **Stat** panel shows Splunk daily ingest GB; a red line warns at 75 % license.

**Swahili proverb**  

:::proverb  
> â€œAkiba haiozi.â€ â€” *Savings never rot.* Store only the metrics you need.  
:::

![Alt text](images/ch06_p12_cost_capacity.png){width=600}

---

## Self-Check Table  

| Concept | Question | Your Answer |
|---------|----------|-------------|
| TLS Flag | Which Prometheus flag enables TLS scrape? | |
| p99 Panel | Grafana panel type best for latency? | |
| SPL Join | Key field to correlate logs & traces? | |
| Burn-Rate | Slack alert multiple & window? | |
| RBAC | Role allowed dashboard edits? | |

---
