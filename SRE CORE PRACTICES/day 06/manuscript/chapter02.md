# Chapterâ€¯2â€¯â€“â€¯Understandingâ€¯theâ€¯Userâ€¯Lens


> **Ava checkâ€‘in:** *â€œMetrics without a user are numbers without contextâ€”today we graft the context back on.â€*

---

## 1â€¯Â·â€¯Reality Check: The Forgotten Customer 

**ðŸŽ¯â€¯Learningâ€¯Objective:** Diagnose the hidden delta between infrastructure health indicators and realâ€‘world customer experience.

**âœ…â€¯Takeaway:** If a single user sees an error while every host shines green, your observability stack is lyingâ€”fix the lens, not the user.

---

### 1.1 Morning Commute, Missionâ€‘Critical Airtime

07â€¯:â€¯58, Haile Selassie Avenue. **Linet Mwangi** juggles one phone, one paper coffee cup, and a backpack strap. Her telco bundle expired overnight; toppingâ€‘up airtime before an 08â€¯:â€¯30 Zoom call with a London client is nonâ€‘negotiable. She opens the â€œQuickPayâ€ widget, hits **PayÂ KESÂ 250**, and watches the spinner pirouette. Five seconds later a red toast slides down: **â€œPayment failed â€” try again later.â€**

Two floors above the Matunda Bank atrium, the **NOC wall** cycles metrics:

| Metric | Value | Status |
|--------|-------|--------|
| CPU util | 41â€¯% | âœ… |
| JVM heap | 58â€¯% | âœ… |
| Disk latency | 1.8â€¯ms | âœ… |
| 5xx per minute | 0.01â€¯% | âœ… |
| Synthetic /healthz | 200Â OK | âœ… |

A junior analyst, **Kevin**, logs the hour as â€œStableâ€.

> *Footsteps.*  Ava bursts in, travel mug in hand, dreadlocks tied back, a rubber band of postâ€‘it notes around her wrist. She points at the pristine wall.  *â€œStable for whom? Linet is southbound on the 43 bus tweeting #MatundaFail.â€*  Wristâ€‘slap.

### 1.2 The Invisible Failure Path

Kevin queries *fraudâ€‘checkâ€‘service* logs:  
```
07:58:22Z  WARN FraudScore timeout 1500ms user_id=988721 trx_id=77aaâ€¦
07:58:23Z  ERROR FraudScore failed after retries user_id=988721 â€¦ HTTP 502
```
The median compute host never broke a sweat; the *edge* spent the error budget.

**Failure cascade:**
1. Fraudâ€‘score model in GPU pool caused GPU context switch spike.  
2. Fraudâ€‘check requests timedâ€‘out at 1â€¯500â€¯ms; API gateway retried twice.  
3. After 3â€¯000â€¯ms gateway returned 502.  
4. Mobile SDK interpreted 502 as fatal, no internal retry.  
5. Linetâ€™s UX displayed generic error.

**Cost of a blind spot:** KESÂ 250 seems trivial; but 38â€¯000 similar failures per hour correlate to ~KESÂ 9â€¯M lost interchange and immeasurable churn.

{{WISDOM_BOX}}

> **SRE WisdomÂ #12 â€”** *â€œIf the customer felt it but your dashboard didnâ€™t, your dashboard is wrong.â€*Â â€”â€¯AvaÂ Kimani

### 1.3 Quick Audit â€“ Are You Measuring Users or Hosts?

| Question | If **No** â†’ Action |
|----------|-------------------|
| Do you record endâ€‘toâ€‘end latency perceived by SDK? | Add clientâ€‘side spans via OpenTelemetry. |
| Do you tag logs with `user_id` or `journey_id`? | Inject correlation IDs. |
| Do you store perâ€‘journey success %, not perâ€‘API? | Aggregate spans into traces. |
| Does your NOC wall surface p99 *journey* latency? | Replace host widget with RED/USE dashboards. |

Take 15â€¯minutes: answer **Yes** or **No**; each **No** is a reliability debt lineâ€‘item.

---

## 2â€¯Â·â€¯Mapping the Endâ€‘toâ€‘End Journey 

**ðŸŽ¯â€¯Learningâ€¯Objective:** Construct a canonical service map that captures every hop, protocol, and datastore in a critical banking workflow, and attach observability hooks aligned to that map.

**âœ…â€¯Takeaway:** Journeys donâ€™t stop at microâ€‘service borders; SILOs in code equal SILOs in telemetryâ€”make the map first, then the metrics.

![Journey map](images/ch02_p01_customer_journey.png){width=550px}

### 2.1 Swimâ€‘Lane Anatomy

| Lane | Component | Typical Latency Budget | Failure Modes | Primary Hook |
|------|-----------|------------------------|---------------|--------------|
| **Device** | SDK + TLS handshake | 50â€¯ms | DNS cache miss, captive portal | RUM beacon (`first_paint`, `ttfb`) |
| **Edge / CDN** | TLS terminate, WAF | 20â€¯ms | Rule misâ€‘match 403 | CDN Realâ€‘User Monitoring logs |
| **API Gateway** | AuthN+AuthZ, routing, rateâ€‘limit | 40â€¯ms | expired JWT, quota breach | Counter `auth_fail_total` |
| **Businessâ€‘logic svc** | Fraudâ€‘check, scoring | 150â€¯ms | GPU stall, model upâ€‘rev | Histogram `svc_latency_bucket` |
| **Ledger** | ACID DB commit | 40â€¯ms | lock contention | Waitâ€‘events via pg_stat_activity |
| **Cache + MQ** | Redis, Kafka | 5â€¯ms | saturation, backpressure | `redis_cmd_duration_seconds` |
| **Notification** | SMS vendor, APNs | 100â€¯ms | vendor 5xx, throttling | External SLI via webhooks |
| **Client Confirmation** | Toast, push notif | 0â€¯ms (async) | APNs token invalid | Delivery receipts |

Summed ideal budget: 405â€¯ms â€” still under the 500â€¯ms aspirational SLO. *Everything else is latency tax.*

### 2.2 Visualising Critical Paths

Ava introduces **MermaidÂ sequence diagrams** rendered in GrafanaÂ 9.  Example:
```mermaid
sequenceDiagram
  autonumber
  participant U as UserÂ SDK
  participant G as APIÂ Gateway
  participant F as FraudSvc
  participant L as LedgerDB
  participant N as NotifySvc
  U->>+G: POST /pay
  G->>+F: score(trx)
  F-->>-G: score=0.7 (1â€¯200â€¯ms)
  G->>+L: INSERT trx
  L-->>-G: 201 (45â€¯ms)
  G->>+N: push_confirm
  N-->>-G: 200 (120â€¯ms)
  G-->>-U: 200 OK (totalÂ 1â€¯415â€¯ms)
```
Grafana renders spans colourâ€‘coded by time; any hop >Â 400â€¯ms glows amber.

### 2.3 Tagging Spans and Metrics

Avaâ€™s rule: *â€œIf you canâ€™t join it, you canâ€™t correlate it.â€* Mandate:
- Every span carries `journey_id`, `user_tier` (e.g., gold vs basic), `segment` (fraud, ledger).  
- Metrics are labelled identically (`{journey_id="$id", segment="fraud"}`).  
- Logs include the same IDs for downstream analysis.

**Prometheus exemplar support** ties the span ID back to trace UI with one click.

### 2.4 Observability Hook Examples

**Fraudâ€‘check latency histogram**
```promql
histogram_quantile(0.99, sum(rate(fraud_score_duration_seconds_bucket[5m])) by (le,segment))
```
**Endâ€‘toâ€‘end SLI**
```promql
sum(rate(journey_complete_total{status="success"}[5m]))
  /
sum(rate(journey_complete_total[5m]))
```
**Capacity alert for GPU saturation**
```promql
max_over_time(gpu_mem_used_bytes[1h])
  / max_over_time(gpu_mem_total_bytes[1h]) > 0.9
```

### 2.5 *Try This* â€” Journey Gap Analysis
1. Export your top 10 revenueâ€‘impact workflows from product analytics.  
2. Draw a swimâ€‘lane map; annotate observability hooks.  
3. Score each hop **0 = none**, **1 = host metric only**, **2 = hop metric**, **3 = userâ€‘visible SLI**.  
4. Total <Â 70? You have blind spots; prioritise instrumenting by revenue weight.

{{TRY_THIS}}

---

## 3â€¯Â·â€¯Where Metrics Lie 

**ðŸŽ¯â€¯Learningâ€¯Objective:** Deconstruct common telemetry fallacies and implement statistical guardâ€‘rails (percentiles, histogram buckets, SLO windows) that expose user pain.

**âœ…â€¯Takeaway:** Averages flatter spikes, synthetic probes skip complexity, and host dashboards sedate incident response; percentiles, burnâ€‘rates, and userâ€‘journey SLIs restore truth.

### 3.1 Mean Time to Mislead

Imagine five payments: 100â€¯ms, 110â€¯ms, 105â€¯ms, 4â€¯900â€¯ms, 5â€¯100â€¯ms.  
**Mean:** 2â€¯263â€¯ms  
**Median:** 110â€¯ms  
If SLO criterion = p50 <â€¯300â€¯ms youâ€™d celebrate; 40â€¯% of your customers rageâ€‘quit.

Ava runs a Jupyter notebook projecting churn probability vs 99thâ€‘percentile latency. The curve is exponential: every 250â€¯ms after 1â€¯s doubles attrition in mobile eâ€‘commerce.

### 3.2 Histogram Hygiene

Prometheus defaults: 0.005, 0.01, 0.025, 0.05, â€¦ but banking latencies hover 200â€“2â€¯000â€¯ms. Buckets misâ€‘represent heavy tails. Avaâ€™s recipe:
```yaml
buckets: [0.05,0.1,0.15,0.2,0.35,0.5,0.75,1,1.5,2,3,5]
```
Then `histogram_quantile(0.995, â€¦)` provides real 99.5th percentile.

### 3.3 Multiâ€‘Window, Multiâ€‘Burnâ€‘Rate Alerts

Host alerts fire on CPU >â€¯90â€¯% for 5â€¯m. Useless. Instead:
```yaml
- alert: PaymentLatency_BurnRate
  expr: (
      sum(rate(journey_latency_bucket{le="2"}[1m]))
        / sum(rate(journey_latency_count[1m]))
    ) < 0.993
  for: 2m  # fast window
- alert: PaymentLatency_BurnRate
  expr: (
      sum(rate(journey_latency_bucket{le="2"}[1h]))
        / sum(rate(journey_latency_count[1h]))
    ) < 0.993
  for: 1h  # slow window
```
PagerDuty triggers only when both windows breachâ€”cuts noise by 80â€¯%.

### 3.4 Synthetic Limitations

Pingâ€‘dom pings `/healthz` every minute. Fraudâ€‘check, ledger, SMS vendor, and GPU anomalies never execute. Ava duplicates synthetic production traffic, including RSA signing, to a shadow environment; same JWT, rate limits, caches. Synthetic failure rates track realâ€‘user failures within Â±3â€¯bps.

**Key metric:** `synthetic_to_real_failure_ratio` â€” if synthetic misses >â€¯0.8 of real failures, refine the script.

{{WRIST_SLAP}}

> *â€œIf your synthetic always passes, either your script is naive or your site is down.â€*

---

## 4â€¯Â·â€¯The Four Golden Signals â€“ Banking Edition 

**ðŸŽ¯â€¯LearningÂ Objective:** Operationalise Golden Signals with thresholds and windows that satisfy regulators and customers.

**âœ…â€¯Takeaway:** Finance demands stricter error ceilings and deterministic latency; wire Golden Signals to error budgets, not abstract SLAs.

![Golden Signals](images/ch02_p04_golden_signals.png){width=550px}

### 4.1 Rate â€“ The Pulse of Revenue

Define *effective transaction rate* (**ETR**): `successful_trx / window`. Low ETR at constant inbound volume => hidden errors. Ava sets a critical alert when ETR drops by 30â€¯% in 5â€¯min *and* burn rate >â€¯2â€¯Ã—.

### 4.2 Errors â€“ Weighted by Value

Not all failures equal. Ava assigns **impact weights**:
- KESÂ 0 â€“ 5Â KÂ trx failure â†’ weightâ€¯1  
- KESÂ 5Â K â€“ 250Â K â†’ weightâ€¯3  
- >â€¯250Â K â†’ weightâ€¯9  
Error SLI calculates `sum(weighted_failures)/sum(weighted_total)`.

### 4.3 Duration â€“ Customer Tolerance Curve

A/B tests show 80â€¯% of premium customers abandon at 1.8â€¯s. Ava sets dual SLO: p99 <â€¯2â€¯s and p90 <â€¯1.2â€¯s. Faster cohort keeps VIP churn <â€¯0.5â€¯% / month.

### 4.4 Saturation â€“ Beyond CPU

Key saturations:
- **Database connection pool** (max 500). Alert at 85â€¯% for 10â€¯s.  
- **Kafka partitions lag**. Lag >â€¯1â€¯K messages for 1â€¯min triggers scaleâ€‘out.  
- **SMS vendor TPS quota**. Expose via pull exporter; page at 90â€¯%.

### 4.5 Errorâ€‘Budget Integration

For each signal define **budget depletion rate**:
```math
burn_rate = error_minutes / ( budget_minutes / period_elapsed )
```
Ava demos a Grafana panel: Burn rate 4â€¯Ã— means budget will exhaust in 7â€¯h; CI pipeline autoâ€‘sets `canary=true` to false.

{{NAIROBI_PROVERB}}

> *â€œMtaka cha mvunguni sharti ainame.â€* (Whoever wants what is under the bed must bend for it.)  
> **Meaning:** To reach lower latency and higher trust, you must bow to disciplined measurement.

---

## 5â€¯Â·â€¯Designing First SLIs 

**ðŸŽ¯â€¯LearningÂ Objective:** Produce productionâ€‘ready SLIs, align them with SLOs, compute budgets, and integrate into CI/CD.

**âœ…â€¯Takeaway:** Good SLIs are SMART (Specific, Measurable, Actionable, Reliable, Timely) and live in codeâ€”never spreadsheets.

![SLI Checklist](images/ch02_p05_sli_checklist.png){width=550px}

### 5.1 SLI Schema

```json
{
  "sli_id": "payment_latency_lt_2000ms",
  "description": "p99 latency for endâ€‘toâ€‘end payments < 2 s",
  "measurement": {
    "source": "Prometheus",
    "query_good": "sum(rate(journey_latency_bucket{le=\"2\"}[5m]))",
    "query_total": "sum(rate(journey_latency_count[5m]))"
  },
  "ownership": {
    "team": "Payments SRE",
    "contact": "#paymentsâ€‘sreâ€‘alerts"
  }
}
```
Checked into Git; CI prevents merge if query_total absent.

### 5.2 Rolling SLI Audit

Cron job `sliâ€‘lint` reads each SLI, executes queries, reports:
- **cardinality** (label explosion >â€¯10â€¯K = warning)  
- **data freshness** (stale >â€¯15â€¯m = critical)  
- **volatility** (stddev >â€¯10â€¯Ã— median = noisy)  
Weekly MR posted with improvement suggestions.

### 5.3 PromQL Pattern Library

| Pattern | Purpose | Snippet |
|---------|---------|---------|
| RED success | Rate of good vs bad | `sum(rate(http_requests_total{code=~\"2..\"}[5m]))` |
| USE saturation | Resource utilisation | `node_filesystem_avail_bytes / node_filesystem_size_bytes` |
| Deduplicated errors | Collapse retries | `sum(rate(errors_total{retry="false"}[5m]))` |
| Clientâ€‘side RUM | JS beacon percentile | `histogram_quantile(0.95, â€¦)` |

Ava insists each new service selects patterns from library; no bespoke snowflake queries without review.

{{ERROR_BUDGET_METER}}

Current meter now reflects SLI #1 (latency) and SLI #2 (weighted errors); Jenkins pushes status to Confluence daily.

---

## 6â€¯Â·â€¯Mini Case Study & Exercise 

**ðŸŽ¯â€¯LearningÂ Objective:** Apply multiâ€‘signal burnâ€‘rate analysis and fast rollback strategy to regain error budget.

**âœ…â€¯Takeaway:** A live incident is a classroomâ€”capturing metrics before, during, after is the exam review that cements learning.

### 6.1 Mpesaâ€‘Lite Incident Timeline

| Time | Event | Metric change | Action |
|------|-------|---------------|--------|
| 10:00 | Feature flag 10â€¯% traffic | p99 latency +140â€¯ms | Observe |
| 10:07 | Twitter complaints start | error SLI deteriorates 0.1â€¯â†’â€¯0.6â€¯% | Alert (burn rate 3â€¯Ã—) |
| 10:09 | Decision meeting | **Stop canary**  | flag to 0â€¯% |
| 10:14 | Latency drops | p99 back to 450â€¯ms | Start rootâ€‘cause |
| 10:25 | GPU saturation found | 95â€¯% util | Scale GPU pool |
| 10:40 | Reâ€‘canary at 5â€¯% traffic | burn rate 0.2â€¯Ã— | Monitor |
| 11:15 | Full traffic | Latency steady | Close incident |

### 6.2 Exercise â€“ Your Turn

1. Simulate traffic burst with `hey` generating 300â€¯RPS to fraud service.  
2. Inject 600â€¯ms delay via `tc qdisc add`.  
3. Observe SLI changes in Grafana.  
4. Adjust threshold from 2â€¯s â†’ 1.2â€¯s; watch how error budget depletes faster.  
5. Record timeline in the table format above.

![Case fix](images/ch02_p06_case_fix.png){width=550px}

**Reflection questions**
- Did your burnâ€‘rate alert fire before Twitter?  
- How many budget minutes did you burn?  
- How would you automate rollback next time?

{{TRY_THIS}}

