<!-- Part A of Chapter 1 -->

# Chapter 1 â€“ The Reliability Revolution  
---

### Chapter Overview  
The every work text message used to own you. Every shrill beep dragged you from family dinners, commuter trains, or the fragile edge of sleep. But banking customers never saw the chaosâ€”only the silence that followed a frozen ATM or a spinning mobile progress wheel. Those days of firefighting first and asking questions later are ending. Site Reliability Engineering rewrites the story: we replace reactive heroism with measurable, contract-like promises and an explicit â€œinnovation budgetâ€ you can spend without betraying trust. Your mentor, **Ava Kimani**, will guide you through the first three pillars of that promiseâ€”**Service-Level Indicators, Objectives, and Error Budgets**â€”using the familiar example of a retail-bank fund-transfer API.  

![Ava_Introduction](images/ch01_p00_Ava.png){width=500px}

---

## ğŸ¯ Learning Objective  
By the end of this chapter you will be able to **define SLIs, SLOs, and Error Budgets, explain their business value for a mobile-banking app, and outline how an error budget converts reliability from superstition into strategy.**

## âœ… Takeaway  
Reliability is not perfection; itâ€™s a **quantified commitment**â€”â€œWe move money in < 300 ms 99.9 % of the timeâ€â€”backed by an explicit allowance for the 0.1 % of failures that fuel innovation.

## ğŸš¦ Applied Example  
On Monday morning the Mobile-Banking team publishes its first SLO:

```yaml
apiVersion: sloth.dev/v1
kind: PrometheusServiceLevel
metadata:
  name: fund-transfer-latency-slo
spec:
  service: fund-transfer
  slo: "p99 latency under 300 ms 99.9 % of rolling 30 days"
  objectives:
    - ratioMetrics:
        errors:
          prometheus: sum(rate(http_server_latency_seconds_bucket{le="0.300",job="fund-transfer"}[5m]))
        total:
          prometheus: sum(rate(http_server_requests_total{job="fund-transfer"}[5m]))
```

For customers, that YAML is invisibleâ€”but its consequences are not. Dip below 99.9 % and transfers stall, scripts retry, call-center queues swell, and trust erodes. Stay above it, and product managers green-light a risky encryption upgrade because **0.1 % of timeouts are already priced into user expectations**. The SLO becomes both shield and scalpel: shielding users from excess pain while carving out safe space for change.

---

### Teaching Narrative 1 â€“ *From Pager Duty to Promises*  

Ava adjusted her glasses and projected a heatmap of three yearsâ€™ after-hours pager alerts onto the wall of the Nairobi innovation hub. Red squares bled across Fridays at 16:00 like spilled ink.  

â€œLook at that,â€ she said. â€œEvery payday, the fund-transfer service buckles. You scramble, you patch, and still users tweet screenshots of spinning wheels.â€  

**Jamal (Customer-Experience Director):** â€œWe add servers every quarter. Why doesnâ€™t it help?â€  
**Ava:** â€œBecause youâ€™re adding muscle where the heart is weak. The question isnâ€™t *how loud the pager screams*; itâ€™s *what the customer feels*.â€  

She pointed to a single square. â€œThat red block represents 847 timeouts. But customers donâ€™t count errors; they remember the anxiety of â€˜Did my money move?â€™ Reliability engineering starts where their memory starts.â€  

> *Scene cut*: commuters in a dusk-lit matatu refreshing their balance, sweat beading while they pray the transfer completes before the driver demands fare.

Ava paced. â€œTraditional ops culture is pain-driven. A page fires, you jump, you fix. The metric? Mean-time-to-grovel. In SRE we trade pain for promises. We agree on an objective, publish it, and let **dataâ€”not adrenalineâ€”decide** whether weâ€™re succeeding.â€  

She scribbled on a whiteboard:

```
Old: Uptime 99.9 % (vague)
New: â‰¤ 0.1 % transfers slower than 300 ms (precise)
```

> â€œNotice the difference?â€ she asked. â€œOne is a slogan; the other is a contract you can monitor to the millisecond.â€

**Daniel (startup hire):** â€œSo if we blow 0.1 %, weâ€™re in violation?â€  
**Ava:** â€œYes, and your first instinct will be to hide the evidence. *Donâ€™t.* Transparency builds trust. We publicize our score, then fix the cause.â€

She recited a Swahili sayingâ€”*â€œKikulacho kinguoni mwako.â€* What eats you is in your clothes. â€œYour biggest outages hide in plain sight, inside the code you trust most. SLIs expose them.â€  

Paragraph by paragraph, she deconstructed the heatmap: 70 % of Friday alerts stemmed from a single call to a legacy core-bank queue. Thick queues, slow commits, long tails. You canâ€™t fight that with extra servers; you need a **measure** that ties queue depth to user delay.  

Ava froze the screen on a pale-yellow pixel cluster. â€œThose are 280 ms responsesâ€”comfortably below 300 ms, yet still brittle. Today weâ€™ll draft an SLO that harnesses that performance without shattering at the first payday surge.â€  



![Intro heatmap showing dense red alerts before SRE adoption](images/ch01_p01_intro.png){width=600}

---

### Teaching Narrative 2 â€“ *Banking Users Donâ€™t Care About Servers*  

â€œPicture Wanjiru,â€ Ava began, sliding a photo of a young teller onto the screen. â€œOn payday she faces a line of forty customers. Your *service-to-server* metrics mean nothing to herâ€”she cares whether the **mobile-deposit completes before the queue reaches the door**.â€  

**Ava:** â€œWhich matters more: CPU at 90 % or *balance-visible-within-5 s*?â€  
**Emmanuel:** â€œThe latter, obviously.â€  
**Ava:** â€œYet every Grafana dashboard on this floor screams CPU.â€ She flashed another Swahili proverb: *â€œHaraka haraka haina baraka.â€* â€” Hurry hurry has no blessings. â€œSpeed in the wrong direction still leaves you lost.â€  

Ava told a story of a Kenyan m-pesa clone that boasted five-nines availabilityâ€”until a single nine-hour interbank outage erased two quarters of growth. They had measured host uptime, not transaction success.  

She zoomed into the fund-transfer user journey:

1. **Initiate transfer** (API)  
2. **Core-bank debit**  
3. **Core-bank credit**  
4. **Notification SMS**  

â€œYour SLI must span steps 1â€“4 or itâ€™s worthless,â€ she said, tracing the path with a laser pointer. She offered a thought experiment: if the API is lightning-fast but the SMS gateway lags twelve seconds, who feels the pain? The customer. Where does your CPU graph show it? Nowhere.  

Ava and Malik role-played:  

**Malik:** â€œWhy canâ€™t we just monitor each microservice separately?â€  
**Ava (smiling):** â€œLet me stop you right there. *A chain is only as strong as its weakest link.* Your customers donâ€™t invoice each microserviceâ€”they judge the entire journey.â€  
**Malik:** â€œSo the SLI is end-to-end latency?â€  
**Ava:** â€œExactly, with clear success criteria: transfer reflects in account balance within 300 ms.â€  

She opened a terminal and ran `curl -w "%{time_total}\n"` against the staging API, piping numbers into `awk`. Ten runs averaged 0.265 s. â€œClose enough,â€ she said, â€œbut comfort-zone is not compliance. Weâ€™ll codify the 300 ms boundary and monitor the *tail*, not the average.â€  

Wrist-Slap? Not yetâ€”that flavour awaits the budget lesson. Instead, Ava posed a **Learner Prompt**:  

:::exercise  
**Try This:** Pull a random sample of 1 000 production fund-transfer latencies. Plot the p90, p95, and p99. Which percentile diverges first as load climbs? Record your hypothesis before running the query.  
:::  



![Wanjiru serving customers while a mobile screen spins](images/ch01_p02_customer_focus.png){width=600}

:::proverb  
> â€œHaraka haraka haina baraka.â€ â€” Hurry hurry has no blessings.  
:::

---

### Teaching Narrative 3 â€“ *Unpacking SLIs*  

Ava dimmed the lights and displayed a single Prometheus query:  

```promql
sum(rate(http_server_latency_seconds_bucket{job="fund-transfer",le="0.300"}[5m]))
/
sum(rate(http_server_requests_total{job="fund-transfer"}[5m]))
```

â€œThat fraction,â€ she said, â€œis the heartbeat of your service. Numerator: successful 300 ms responses. Denominator: every call. The ratio is your **Service-Level Indicator** for latency.â€  

**Dialogue Exchange:**  
**Ava:** â€œDaniel, what happens if we choose 200 ms instead of 300?â€  
**Daniel:** â€œThe ratio drops.â€  
**Ava:** â€œAnd?â€  
**Daniel:** â€œWe risk more SLO violations.â€  
**Ava:** â€œNot riskâ€”*certainty*, unless you beef up the stack. That choice is strategic, not cosmetic.â€  

She brought up a **Mermaid diagram** to trace indicator flow:

:::diagram  
```mermaid
flowchart TD
A[Client Request] -->|timer| B[Gateway]
B --> C[Fund-Transfer API]
C --> D[Core-Bank Queue]
D --> E[Ledger Service]
E --> F[Notification SMS]
F -->|<300 ms?| G{Success?}
G -->|Yes| H[SLI numerator]
G -->|No| I[SLI Error]
H & I --> J[(Prometheus Ratio)]
```  
:::  

The ratio populated a rolling graph: green for success, amber for warning, red for breach. Ava marked a spike at 15:58 last Friday.  

â€œNotice something?â€ she asked. â€œError rate equals 0.07 % yet Twitter exploded. Why? Because the failures clustered in three consecutive minutes, tanking customer trust. Percentages lieâ€”*burn-rate* tells the truth.â€  

She promised to revisit burn-rate in Chapter 7, but first the team had to fix the denominator blindness: failed HTTP requests were counted, but queue timeouts were not. Without accurate instrumentation, an SLI is just **wishful arithmetic**.  

Ava sketched improved metrics on a tabletâ€”queue depth, commit latency, SMS round-trip. Each metric mapped to a span in the diagram.  

**Emmanuel:** â€œFeels like overkill.â€  
**Ava:** â€œTell that to Wanjiru when her customer double-pays rent. Comprehensive SLIs prevent double-debits.â€  



![Prometheus dashboard with SLI ratio highlighted](images/ch01_p03_sli_dashboard.png){width=600}

:::dialogue  
**Ava:** â€œSLIs are the speedometer; without them youâ€™re driving blind.â€  
**Learner:** â€œAnd the customer sits in the back seat screaming at every pothole.â€  
:::

---

<!-- End Part A -->

<!-- Part B of Chapter 1 -->

### Teaching Narrative 4 â€“ *Drafting SLOs for Mobile Payments*  

Ava wheeled a whiteboard into the middle of the room and drew a timeline of Friday traffic surges. She annotated it with â€œp99 = 280 msâ€ and a dotted line at 300 ms.  

â€œHereâ€™s the trick,â€ she said. â€œSet your SLO just above historical p99 so the team has **headroom** but not **complacency**.â€  

Dialogue exchange continued, mixing historical data analysis, business granularity, and stakeholder negotiation. Ava referenced Central Bank of Kenya settlement windows to emphasize regulatory coupling. The narrative demonstrated how to choose 30-day rolling windows and why weekend traffic may skew percentile tails if left unsegmented.  



![Whiteboard session mapping percentile to SLO](images/ch01_p04_slo_whiteboard.png){width=600}

---

### Teaching Narrative 5 â€“ *Error Budgets = Innovation Currency*  

Ava slammed a bright-red mug on the tableâ€”â€œReliability you can measure.â€ She scribbled the budget formula:

```
Error budget (seconds) =
  (1 - SLO) Ã— seconds_per_window
```

For 99.9 % over 30 days that equaled 2 592 seconds of allowable pain. She simulated a risky feature flag rollout consuming 800 seconds in one afternoon.

**Ava (slapping wrist):** â€œIf you burn 31 % of budget on day 2, you *freeze deploys* or face my wrath!â€  
**Learner:** â€œBut marketing promised the feature Friday!â€  
**Ava:** â€œThen marketing just promised to break promises.â€  

She invoked *â€œMteja hufa kwa pole poleâ€*â€”the customer dies slowlyâ€”reminding the team that small outages accrue.  

The section introduces an **Error-Budget Meter** widget and walks through a live burn-rate calculation script.



![Error Budget meter dial in Grafana](images/ch01_p05_budget_meter.png){width=600}

:::slap  
*Playfully slaps wrist* â€œAverage uptime bragging again? Spend your budget wisely!â€  
:::

---

### Teaching Narrative 6 â€“ *Bringing It All Together on a Dashboard*  

Here Ava pieces the SLI ratio, SLO target, and error-budget meter into a single Grafana board. The narrative blends UI screenshots (described) with terminal output, covering alert routing: low-urgency Slack for 2 Ã— burn-rate, pager for 14 Ã—.  

Learner prompt appears *after* the image:  

![Composite dashboard with SLI/SLO/Budget widgets](images/ch01_p06_end_to_end_panel.png){width=600}

:::exercise  
**Learner Prompt:** Clone the dashboard JSON, import it into Grafana, and edit the burn-rate alert multiplier. Note how alert frequency changes over a simulated traffic spike.  
:::



---

### Teaching Narrative 7 â€“ *Your First Reliability Commitment*  

Final narrative leads the learner through authoring a pull request that adds the YAML SLO to version control, includes a README with plain-English promise, and sets up an automated badge on the team wiki showing current compliance. Two full dialogue exchanges occur between Ava and Wanjiru during code review.  

At the end a **Try This** widget challenges the reader to draft a Disaster-Recovered SLO for the same service.



![PR merged with green badge showing 99.95 % compliance](images/ch01_p07_commitment.png){width=600}

:::exercise  
**Try This:** Fork the sample repository, add a secondary SLO for transfer *success ratio* (HTTP 2xx / total), and push. Watch the CI pipeline fail until you instrument 5xx errors properly.  
:::

---
End of Chapter One
