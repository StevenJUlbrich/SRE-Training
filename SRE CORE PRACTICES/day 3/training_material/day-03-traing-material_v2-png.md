# Day 3 SRE training module

Below is a comprehensive, narrative-driven Day 3 SRE training module. Itâ€™s written as if youâ€™re guiding an interactive workshop, weaving in real-life war stories (multi-page style), humor, and practical code snippets. It remains vendor-neutral, includes concise Python examples, and addresses a broad range of organizational sizes. Each section is presented in a storytelling format, with tiered content for beginners, intermediates, and advanced SREsâ€”plus plenty of humor and humanity along the way!

---

## From Observability Warrior to SRE Superhero!**

### **Introduction: Evolving from Observability Wizard to SRE Champion**

**Storytime Setup**  
Picture this: Youâ€™re huddled around a flickering conference-room projector, sipping stale coffee at 2 AM. The logs are *apparently* telling you something, but no oneâ€™s sure exactly what. Your incident management tool is screaming, your manager is demanding an ETA, and youâ€™re remembering Day 1 and Day 2 of this courseâ€”where you gained new superpowers in observability. Now, on Day 3, itâ€™s time to step it up. Youâ€™re not just an Observability Wizard anymoreâ€”youâ€™re about to become a full-blown SRE Champion, turning all that data into actionable reliability.

**Key Themes**  
- Youâ€™ve got the tools from Day 1 and Day 2. **Now** youâ€™ll learn to use them as an SRE does.  
- Transform your mindset: not just â€œIs it broken?â€ but â€œHow broken is it, does it matter, and whatâ€™s our plan?â€  
- Learn the delicate art of error budgets, where â€œ100% uptimeâ€ is the boogeyman, not the objective.  
- Appreciate that SRE transformations can be messyâ€”real life is seldom perfect.  

**Video Placeholder**  
{{VIDEO_LINK_INTRO}}

*(Weâ€™ll add the actual link laterâ€”no worries!)*

---

## **Section 1: The Art & Science of SLOs**

Practical Incident Workflow (Sample)**



![Mermaid Diagram: flowchart](images/diagram-1-8bb2a2b3.png)



**Short YAML Snippet for Incident Routing**:
```yaml
incident_management:
  severity:
    SEV1: "Critical outage, immediate response"
    SEV2: "Major degradation, respond in <30 minutes"
  auto_escalation:
    - engineering_lead
    - sre_lead
  runbook_links:
    - service: "payment_gateway"
      doc: "http://docs.internal/runbooks/payment_incident.md"
```


### **1.1 Beginner Level (ğŸ”): SLO Foundations**

#### **Main Narrative: â€œThe Tale of Sam and the Metric Tsunamiâ€**  
Letâ€™s meet Sam, a passionate engineer who joined â€œGenericCoâ€ right after finishing the Day 2 observability training. Sam wanted *all* the metricsâ€”CPU usage, memory usage, disk I/O, network throughput, the cafeteriaâ€™s coffee machine temperature. The problem? Sam ended up drowning in a sea of data.

1. **Translating Raw Metrics into SLIs**  
   - **Samâ€™s Dilemma:** Even with a million metrics, Sam couldnâ€™t *prioritize*.  
   - **Guiding Principle:** Focus on service-level indicators that capture *user pain*: e.g., request latency, error rates, or success rates.  

2. **Setting Realistic SLO Targets**  
   - â€œ**Perfect** is the enemy of **reliable**.â€ At first, Sam tried 99.999% for everything.  
   - After an all-nighter chasing a fraction-of-a-percent drop in availability, Sam realized 99.9% might be enough.  

3. **Understanding SLIs, SLOs, SLAs**  
   - **SLI**: The *metric*, like request success rate.  
   - **SLO**: The *target*, like â€œ99.9% of requests succeed.â€  
   - **SLA**: A *contract* with external parties, with penalties if you fail.  

4. **Your First Error Budget**  
   - **Samâ€™s Lightbulb Moment:** The 0.1% downtime (for a 99.9% SLO) gave the team permission to innovate without fear of *tiny* incidents.  

5. **Avoiding the â€œToo Many Metricsâ€ Trap**  
   - Samâ€™s team learned to pick *three to five* key SLIs. More than that, and youâ€™re back to drowning.

#### **Short Python Snippet: Basic Error Budget Calculation**  
```python
import datetime

def calculate_error_budget(slo_percentage, total_requests, actual_failures):
    """Calculate remaining error budget based on an SLO and real data."""
    allowed_failures = total_requests * (1 - slo_percentage)
    used_failures = actual_failures
    remaining = allowed_failures - used_failures
    return remaining

# Example usage:
slo_percentage = 0.999  # 99.9%
total_requests = 100_000
actual_failures = 50

budget_left = calculate_error_budget(slo_percentage, total_requests, actual_failures)
print(f"Remaining error budget: {budget_left} failures allowed.")
```

*(A simple snippet that demonstrates the conceptâ€”Sam used something like this to track whether they were still within that 0.1% error budget.)*

---

### **1.2 Intermediate Level (ğŸ§©): SLO Artistry**

#### **War Story: â€œThe Midnight Alert Sagaâ€**  
Imagine youâ€™re on-call, and your pager (or phone) starts shrieking at midnight. You jump out of bed, scramble for your laptop, only to discover itâ€™s a *minor* blip in one region that your SLO-based alerting overreacted to. Multiply that by 20 times a month, and you have a morale crisis on your hands.

**Key Lessons**  
- **Multi-Dimensional SLOs:** Donâ€™t lump everything together. For instance, measure success rate in each region or data center separately.  
- **Building Smarter Alerts:** Implement burn-rate alerts that trigger only when your error budget is projected to be exhausted *quickly*.  
- **Error Budget Negotiation:** Work with product teams to decide how to spend or protect that error budget. If youâ€™re launching a new feature, you may accept higher risk.  

**Short Python Snippet: Burn-Rate Alert Concept**  
```python
def projected_burn_rate(error_rate, time_window_hours, slo_hourly_budget):
    """
    Estimate how quickly you'd burn your error budget.
    error_rate: fraction of requests failing per hour
    time_window_hours: time period to observe
    slo_hourly_budget: fraction of requests allowed to fail in that hour
    """
    return (error_rate / slo_hourly_budget) * time_window_hours

# For example, if error_rate is 0.003 and your SLO budget is 0.001 per hour:
print(projected_burn_rate(0.003, 6, 0.001))  # Are we burning too fast?
```

---

### **1.3 Advanced/SRE Level (ğŸ’¡): SLO Mastery**

#### **In-Depth War Story: â€œThe SLO Scaling Chroniclesâ€**  
This is a *multi-page* tale of an SRE leader, Alex, who aimed to roll out an SLO framework across a massive, multi-department organization:

1. **Day 1**: The Top-Down Mandate  
   - The CTO excitedly declares, â€œWe need SLOs across every service!â€  
   - The 200+ service owners recoil at the complexity.  

2. **Day 15**: The Political Minefield  
   - Product managers fear that strict SLOs could â€œslow innovation.â€  
   - Some service owners worry about being penalized if they donâ€™t meet targets.

3. **Day 30**: Building a Tiered System  
   - Alex introduces **tiered SLOs**: Tier 1 for mission-critical systems, Tier 2 for important-but-not-critical, Tier 3 for nice-to-have.  
   - This helps focus the highest reliability demands where they truly matter.  

4. **Day 60**: Negotiations and Revelation  
   - Alex organizes reliability workshops, showing how to set an SLO thatâ€™s challenging but *not* impossible.  
   - A few early adopters proudly show 99.5% SLO attainment, with error budgets fueling safe feature experiments.

5. **Day 90**: The Cultural Shift  
   - The org sees fewer 3 AM incidents because theyâ€™re building around realistic SLOs.  
   - Alex wins over the CFO by showing how stable systems reduce hidden costs.  

6. **The Climax**: Predictive Reliability  
   - Advanced teams start forecasting usage spikes and system saturation.  
   - They proactively adjust capacity before the error budget is threatened.

7. **End Result**  
   - SLO frameworks become the â€œnew normal,â€ with standardized dashboards, consistent reporting, and a new *culture* of reliability over perfection.  
   - **Moral:** SLO mastery is not just about technologyâ€”itâ€™s about negotiation, communication, and measured risk-taking.

---

## **Section 2: The Incident Management Dojo**

Example Production Readiness Review Template**



![Mermaid Diagram: flowchart](images/diagram-2-fa80019c.png)



### **2.1 Beginner Level (ğŸ”): Survival Basics**

#### **War Story: â€œThe 2AM Database Disasterâ€ (Multi-Page)**  
Meet Taylor, a brand-new on-call engineer:

1. **Scene 1: The Alert**  
   - Taylor gets woken at 2 AM. The alert says â€œDATABASE ERROR!â€  
   - No clue which database or how critical the error is.  

2. **Scene 2: The Scramble**  
   - The runbook is 200 pages of outdated info.  
   - Team Slack channels are silent because half the team is asleep.  

3. **Scene 3: Lessons Learned**  
   - By the time morning rolls around, the incident is resolvedâ€”*somehow*.  
   - In the retrospective, the team decides to adopt clearer severity definitions (SEV1, SEV2, etc.), better runbooks, and shared on-call rotations.

4. **Takeaways**  
   - **Severity Levels**: Not everything is SEV1.  
   - **Documentation**: Short, relevant runbooks for half-asleep engineers are gold.  
   - **First Postmortem**: â€œNever againâ€ means capturing *root causes*, *remediation steps*, and *lessons learned*.

---

### **2.2 Intermediate Level (ğŸ§©): Incident Mastery**

#### **War Story: â€œThe Tale of the Recurring Outageâ€**  
A mid-sized e-commerce company faced the *same* payment gateway outage every Friday night:

1. **Realization**  
   - A simple ephemeral network blip was bringing down the payment microservice.  
   - Because they had no automated failover or playbook, each time was chaos.  

2. **Transformation**  
   - The team built a **playbook** that quickly guided them to switch traffic to a backup payment gateway.  
   - They added **automation** that recognized the network error and triggered the failover script.  

3. **Result**  
   - The next time it happened, they were back up in minutes.  
   - Alerts became more precise: no more 50-line Slack messages, just a quick â€œFailover triggered for Payment Service.â€

4. **Incident Communication Templates**  
   - They introduced a standard Slack message format for incident updates.  
   - â€œMajor Payment Gateway Outage â€“ Services impacted: Payment, Checkout. ETA to resolution: 10 minutes.â€

---

### **2.3 Advanced/SRE Level (ğŸ’¡): Incident Leadership**

#### **War Story: â€œThe Chaos Engineering Experiment Gone Wrong (But Right)â€**  
At a growing tech startup, an ambitious SRE manager, Jordan, decided to *inject chaos* into the system to see what would happen:

1. **Planning Stage**  
   - Jordan carefully explained chaos engineering to leadership: â€œWeâ€™ll break things intentionally to learn about hidden weaknesses.â€  
   - Leadership raised eyebrows but approved a small experiment.  

2. **Chaos Unleashed**  
   - The experiment forcibly killed 10% of random service instances.  
   - Suddenly, a deeply hidden configuration mismatch brought down the staging environment *and* threatened production.  

3. **Firefighting**  
   - Jordanâ€™s team discovered a hidden dependency that caused a cascading failure.  
   - They documented the scenario in detail.

4. **Learning**  
   - The chaos experiment was â€œgone wrongâ€ because it uncovered a *huge* architectural flaw.  
   - But it was â€œgone rightâ€ because they caught and fixed it before it exploded in production.

5. **Long-Term Impact**  
   - Jordanâ€™s organization embraced chaos engineering, building a robust set of test harnesses for *intentional* breakage.  
   - Incidents became less frequent (and less severe) over time.

---

## **Section 3: Taming the Observability Beast (Cost & Scale)**

Example Cost Analysis Snippet**

```python
import pandas as pd

# Hypothetical monthly usage data
data = {
    'Team': ['Payments', 'Catalog', 'Analytics', 'Mobile'],
    'LogVolumeGB': [700, 300, 1200, 200],
    'MetricsCount': [2000, 1000, 5000, 800]
}
df = pd.DataFrame(data)

# Approximate cost calculations
df['LogCostUSD'] = df['LogVolumeGB'] * 0.10
df['MetricsCostUSD'] = df['MetricsCount'] * 0.02
df['TotalCostUSD'] = df['LogCostUSD'] + df['MetricsCostUSD']

print(df)
```

**Possible Output**:
```ts
        Team  LogVolumeGB  MetricsCount  LogCostUSD  MetricsCostUSD  TotalCostUSD
0   Payments          700          2000        70.0           40.00         110.0
1    Catalog          300          1000        30.0           20.00          50.0
2  Analytics         1200          5000       120.0          100.00         220.0
3     Mobile          200           800        20.0           16.00          36.0
```


### **3.1 Beginner Level (ğŸ”): Cost Control 101**

#### **War Story: â€œThe Month the Logging Bill Explodedâ€**  
- The CFO storms into the SRE team meeting, waving an astronomical cloud logging invoice.  
- Turns out, the team logs *every* request detail in debug mode, 24/7.  
- They adopt **sampling** and a leaner retention policy.  
- The next month, the CFO smiles (somewhat)â€”the bill has dropped drastically.

---

### **3.2 Intermediate Level (ğŸ§©): Advanced Optimization**

#### **War Story: â€œA Tale of Two Servicesâ€**  
- *Critical Service A* has extremely high reliability requirements.  
- *Non-Critical Service B* is internal analytics with flexible SLAs.  
- The SRE team designs **tiered observability**:  
  - High sampling for Service A, minimal sampling for B.  
  - Different log retention based on business importance.  
- This approach slashes data costs without jeopardizing vital troubleshooting data.

---

### **3.3 Advanced/SRE Level (ğŸ’¡): Observability Economics**

#### **War Story (Multi-Page): â€œThe Observability Platform Revolutionâ€**  
Picture a sprawling enterprise with thousands of services. Observability costs are ballooning:

1. **Stage 1: The Crisis**  
   - The CFO demands a 30% cost reductionâ€”*without* losing insight.  

2. **Stage 2: Building an Internal Platform**  
   - An SRE guild sets up â€œObservability-as-a-Service.â€  
   - They define standardized agents, sampling policies, and retention tiers.  

3. **Stage 3: Allocation Models**  
   - Teams now see their monthly usage costs on a shared dashboard.  
   - This transparency leads teams to refine logs and avoid dumping debug data in production.  

4. **Stage 4: Cultural Shift**  
   - Engineers start optimizing their code for fewer, clearer logs.  
   - TCO (Total Cost of Ownership) discussions become normal in project planning.  

5. **Stage 5: Executive Buy-In**  
   - The CFO becomes an ally, praising the SRE teamâ€™s data-driven approach.  
   - Observability transforms from a black hole of spending into a strategic resource.

---

## **Section 4: Reliability Culture: Making SRE a Superpower, Not a Burden**

### **4.1 Beginner Level (ğŸ”): Cultural Foundations**

#### **War Story: â€œThe Toil Monster and the Reluctant Automatorâ€**  
- Devin, an ops engineer, manually restarts a failing service 20 times a day.  
- They track the â€œtoilâ€ and realize theyâ€™re spending half their week on this *one* menial task.  
- By automating the restart process (and fixing the root cause), they free up time to tackle more interesting reliability tasks.  
- The â€œToil Monsterâ€ is slayed, morale rises, and leadership notices the improvement in *true* productivity.

---

### **4.2 Intermediate Level (ğŸ§©): Cultural Growth**

#### **War Story: â€œThe Production Readiness Review That Saved Christmasâ€**  
- An e-commerce company has a massive holiday launch scheduled for December.  
- Initially, dev teams push back: â€œWe donâ€™t have time for your Production Readiness Review.â€  
- They grudgingly fill out a concise PRR checklist anyway, discovering a crucial misconfiguration that would have toppled the site under holiday load.  
- The last-minute fix *saves* the holiday season. Dev teams become PRR converts.

---

### **4.3 Advanced/SRE Level (ğŸ’¡): Cultural Leadership**

#### **War Story: â€œThe Executive Who Learned to Love Reliabilityâ€**  
- At â€œBigCorp,â€ the CFO initially sees SRE as a cost center.  
- An SRE director uses *business-friendly metrics*: customer retention, reduced downtime, brand reputation.  
- Over a year, the CFO sees direct correlation between reliability improvements and increased revenue.  
- Suddenly, the SRE budget *expands*, and reliability becomes a core company value.

---

## **Section 5: Shift-Left Reliability**

### **5.1 Beginner Level (ğŸ”): Development Integration**

#### **War Story: â€œThe Mystery of the Silent Serviceâ€**  
- A new microservice fails quietly in productionâ€”no logs, no alarms.  
- Developers realize they neglected to add even basic error metrics or logs.  
- They integrate minimal observability checks in the **CI/CD pipeline**, ensuring every commit has at least a few essential logs and a health endpoint.  
- Silent failures vanish.

---

### **5.2 Intermediate Level (ğŸ§©): Observability-First Development**

#### **War Story: â€œThe Synthetic Customer Who Saved the Dayâ€**  
- The SRE team sets up synthetic tests that mimic real user journeysâ€”logging in, placing an order, checking out.  
- During a normal code deploy, a synthetic test catches a subtle bug that would have prevented users from checking out.  
- The deploy is halted, the bug is fixed *before* real customers see it, and the on-call staff remain blissfully asleep that night.

---

### **5.3 Advanced/SRE Level (ğŸ’¡): Reliability by Design**

#### **War Story (Multi-Page): â€œThe Great Database Coupling Disaster of 2023â€**  
1. **The Setup**  
   - A large microservices ecosystem with a critical â€œUser Profileâ€ service.  
   - Everything *quietly* depends on the same database cluster.  

2. **The Big Outage**  
   - A small schema change triggers massive lock contention.  
   - Cascading failures bring down half the application fleet.  

3. **SRE Involvement**  
   - After the fiasco, SREs lead a new architecture design that decouples services and uses caching.  
   - Chaos experiments reveal more hidden dependencies.  

4. **Long-Term Transformation**  
   - Teams adopt a principle: â€œNo new service without a reliability design review.â€  
   - Observability-driven dev cycles become standard, preventing future fiascos.

---

## **Section 6: The SRE Journey: Putting It All Together**

### **In-Depth Multi-Page Capstone Story: â€œThe Phoenix Project 2.0: Rise of the SREâ€**

This extended narrative follows a fictional company, **TechCore**, from chaos to SRE excellence:

1. **Chaos Reigns**  
   - TechCore has partial observability from Day 1â€“2 efforts. They *see* metrics but do nothing with them. Incidents are frequent, blame is common, and weekends are ruined.

2. **The First Spark: SLO Adoption**  
   - An ambitious engineer sets an SLO for the user-facing service: 99.5% success rate.  
   - Everyone freaks outâ€”â€œBut we need 100% uptime!â€  
   - They compromise on 99.8%, giving a small error budget.

3. **Incident Management Renaissance**  
   - TechCore invests in a well-defined incident process, leveling alerts to reduce noise.  
   - Postmortems become blameless and everyone shares in root cause analysis.

4. **Observability Scaling**  
   - As the user base grows, so does the data. Logs from thousands of microservices threaten to bankrupt TechCore.  
   - A cost-optimization initiative brings in sampling, trimming pointless debug logs.

5. **Cultural Shift**  
   - Development teams consult SREs *before* launching new features.  
   - Production Readiness Reviews and real-time feedback loops become part of the dev lifecycle.

6. **Reliability by Design**  
   - TechCore invests in chaos engineering game days, anticipating and mitigating potential outages.  
   - They identify single points of failure and redesign architectures for resilience.

7. **Executive Alignment**  
   - Impressed by fewer outrages and improved user satisfaction, leadership invests further in SRE.  
   - SRE matures from â€œfirefighting crewâ€ to â€œstrategic reliability partner.â€

8. **Final Outcome**  
   - TechCoreâ€™s reliability posture leaps forward.  
   - The on-call teams experience fewer late-night pages, and the CFO sees how a stable platform boosts revenue.  
   - Engineers are proud of a culture that values learning and improvement, not blame.

**Lessons Learned**  
- Real SRE transformations involve technical *and* social challenges.  
- Small, incremental steps add up: from metrics to SLOs, from panic to structured incident response, from runaway logs to cost management.  
- Humor and empathy go a long way in building a culture that *truly* cares about reliability.

---

## **Essential Code & Configuration Examples**

Throughout the war stories, youâ€™ve seen short Python snippets demonstrating basic error budget calculations, burn-rate projections, and more. Consider also:

1. **SLO Dashboards**: A minimal config file or snippet that sets up a vendor-neutral dashboard (e.g., using JSON or YAML) to visualize SLO error budgets.  
2. **Incident Routing Logic**: Simple pseudo-code that routes specific alerts to the right on-call engineer.  
3. **Cost-Optimization Scripts**: High-level examples that show how to parse log volume or metrics usage, so teams can see where theyâ€™re burning money.  

*(All remain vendor-neutral but demonstrate the concept in short, readable snippets.)*

---

## **Essential Diagrams with Personality**

1. **The SRE Evolution**: Show a fun timeline from â€œEverythingâ€™s on fireâ€ to â€œWe expected this and have a plan.â€  
2. **SLI/SLO/SLA Pyramid**: Indicate how SLIs feed into SLOs, and SLOs might feed into SLAs.  
3. **Incident Management Flowchart**: A playful â€œWhen an alert fires, do X, Y, Zâ€ diagram with comedic labels like â€œPANIC â€“ if you must, but only for a minute.â€  
4. **Cost Optimization Framework**: Pie charts showing how logs, metrics, and traces can slice up the observability budget.  
5. **SRE Team Models**: Cartoons of different organizational setups (centralized SRE team vs. embedded SREs).  

---

## **Conclusion & Next Steps**

By weaving together **error budgets**, **incident management**, **cost optimization**, **culture change**, and **shift-left reliability**, youâ€™re now armed with the full SRE toolkit. Day 3 is the turning point where observability meets structured SRE discipline. Remember:

- **Start simple**: Donâ€™t overload your SLOs or logging.  
- **Iterate**: Reliability is a journey, not a destination.  
- **Culture matters**: Tools are easy; people are hard.  
- **Stay curious**: Chaos engineering, predictive analysis, shift-left testingâ€”embrace them step by step.  

Embrace your new powers responsibly, champion reliability in your organization, and above allâ€”keep your sense of humor. You might just save your weekends (and your sanity) in the process.

**Final Video Placeholder**: {{VIDEO_LINK_CAPSTONE}}

---

### **You are now an SRE Superhero!**

**Remember:** True heroism isnâ€™t about avoiding every failureâ€”itâ€™s about knowing which failures matter, and having a plan to bounce back stronger. So go forth, SRE Champions, and bring reliability wherever you roam.