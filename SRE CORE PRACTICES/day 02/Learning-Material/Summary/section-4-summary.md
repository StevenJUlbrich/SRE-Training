# ğŸ“– Section 4 â€“ SLIs and SLOs: Turning Metrics into Promises  
> _"Metrics measure behavior. SLIs define accountability."_  
> â€” Marisol Vieira

---

## ğŸ§  Core Concept

An **SLI** (Service Level Indicator) is a metric that reflects *what your users actually care about*.  
An **SLO** (Service Level Objective) is the *target* you aim to maintain for that SLI.  
And when you combine these with alerts, error budgets, and reportingâ€”youâ€™ve created a **reliability system**.

> âœ… SLIs measure *experience*.  
> âœ… SLOs define *expectations*.  
> âœ… Error budgets define *limits*.

This section provides a complete frameworkâ€”from SLI definition to alerting logic and long-term governance.

---

## ğŸ“š Table of Contents for This Section

1. ğŸ” SLI vs. Metric: Whatâ€™s the Difference?  
2. ğŸ“ SLO Design Framework  
3. âš–ï¸ Error Budgets: Enforcing Reliability  
4. ğŸ›  SLI Query Library  
5. âš™ï¸ Alerting & Burn Rate Thresholds  
6. ğŸ“ˆ Monitoring Dashboards for SLIs  
7. ğŸ—º Mermaid Diagrams  
8. âœ… Best Practices

---

## ğŸ” SLI vs. Metric

| Metric Type | Example                               | User-Relevant? | Use as SLI? |
|-------------|----------------------------------------|----------------|-------------|
| System metric | `cpu_usage`, `memory_bytes`           | âŒ             | âŒ          |
| App metric   | `http_requests_total`, `latency_p95`  | âœ… Sometimes   | âœ…          |
| Business metric | `orders_completed_total`, `login_success_rate` | âœ… | âœ… |

> ğŸ“Œ **SLIs are not all metrics, but all SLIs are metrics.**  
> The difference is *intent*.

---

## ğŸ“ Designing an SLO

> ğŸ—º Mermaid: SLI to SLO Design Flow  
```mermaid
flowchart TD
A[Identify Critical User Path] --> B[Define SLI]
B --> C[Write PromQL]
C --> D[Set Target (SLO)]
D --> E[Track Error Budget]
E --> F[Alert & Report]
```

---

## âœï¸ SLO Design Template

| Field           | Description                               | Example                          |
|------------------|-------------------------------------------|----------------------------------|
| **SLI**          | What are you measuring?                   | 95% of checkouts complete <2s   |
| **SLO Target**   | Whatâ€™s acceptable over time?              | 99.9% success over 30 days       |
| **Budget Window**| How long is the rolling period?           | 30 days                          |
| **Budget Size**  | 0.1% (43 minutes per 30 days)             | (computed from SLO)              |
| **Alert Window** | When to notify on burn rate               | Burn rate > 2x for 5m            |

---

## âš–ï¸ Error Budgets

If your SLO target is 99.9%, you are allowed:
- 0.1% of failure
- Over 30 days, thatâ€™s **43 minutes and 12 seconds of downtime**

> This is your **error budget**.  
> Blow it, and you're out of compliance.  
> Respect it, and you stay reliable *and* deploy with confidence.

---

## ğŸ§ª SLI Query Library

### âœ… Availability

```promql
(
  sum(rate(http_requests_total{status=~"2..|3..", job="checkout"}[5m]))
/
  sum(rate(http_requests_total{job="checkout"}[5m]))
) * 100
```
> Measures the percentage of successful responses.

---

### âœ… Latency (p95 or p99)

```promql
histogram_quantile(0.95,
  sum(rate(request_duration_seconds_bucket{job="checkout"}[5m]))
  by (le)
)
```
> Measures request latency at the 95th percentile.

---

### âœ… Error Rate

```promql
sum(rate(http_requests_total{status=~"5..", job="checkout"}[5m])) 
/ 
sum(rate(http_requests_total{job="checkout"}[5m]))
```
> Fraction of requests failing with a 5xx status.

---

### âœ… Throughput / Success Ratio

```promql
sum(rate(orders_completed_total{region="us-east"}[5m]))
```
> Business-level SLI: user transactions per region

---

## âš™ï¸ Alerting with Burn Rate

A burn rate tells you **how fast** youâ€™re consuming your error budget.

| Burn Rate | Meaning                                |
|-----------|----------------------------------------|
| 1         | Budget being used perfectly evenly     |
| >1        | Budget is being consumed *too fast*    |
| <1        | Budget is healthy, safe burn           |

### ğŸ”¥ Burn Rate Alert Example

> **SLO Target**: 99.9% â†’ error budget = 0.001  
> **Alert Trigger**: Burn rate > 2 for 5 minutes

```promql
(
  sum(rate(http_requests_total{status=~"5.."}[1m]))
/
  sum(rate(http_requests_total[1m]))
) / 0.001 > 2
```

> This tells you: "We're using our 30-day budget *twice as fast* as allowed."

---

## ğŸ“Š SLI Dashboard Layout

| Row        | Panels Included                                |
|------------|------------------------------------------------|
| **Top**    | Availability %, Latency (p95), Error Rate      |
| **Middle** | Throughput, Burn Rate                          |
| **Bottom** | Raw metrics for debugging                      |

Add:
- **Thresholds**
- **Annotations for deploys/incidents**
- **Links to alert definitions**

---

## ğŸ—º Mermaid: SLI Dashboard Flow  
```mermaid
graph TD
A[SLI Query] --> B[Visual Panel]
B --> C[Threshold Rules]
B --> D[Linked Alert Rule]
D --> E[Notification System]
```

---

## âœ… Best Practices for SLIs & SLOs

- âœ… Align SLIs with *user experience*, not system internals  
- âœ… Define SLIs before defining alerts  
- âœ… Use histograms for latency, not averages  
- âœ… Donâ€™t try to make 100% your SLOâ€”*allow* for failure  
- âœ… Automate burn rate tracking and reporting  
- âœ… Review SLOs quarterly and revise with actual data  

---

## ğŸ§  Key Takeaways

- SLIs are the lens through which we understand **user experience**  
- SLOs turn those metrics into promisesâ€”internally or contractually  
- Error budgets are operational currency; use them wisely  
- Burn rate tells you when youâ€™re sprinting toward failure  
- Dashboards built on SLIs **justify themselves**

