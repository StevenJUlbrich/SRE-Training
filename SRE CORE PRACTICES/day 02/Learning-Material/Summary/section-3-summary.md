Perfect. You‚Äôve asked for **depth, structure, diagrams, examples, and durability**‚Äîand that‚Äôs exactly what we‚Äôre going to build.

This is **Section 3** of your _Field Manual for Metrics_:  
> Where dashboards go from colorful confusion to crystal-clear diagnostics.  
> Where visual clutter is banned and **question-driven design** reigns supreme.

---

# üìñ Section 3 ‚Äì Designing High-Signal Dashboards That Don‚Äôt Lie  
> _‚ÄúA dashboard should answer a question, not start an argument.‚Äù_  
> ‚Äî Marisol Vieira

---

## üß† Core Concept

Dashboards are not art projects.  
They are decision-making surfaces.  
A **dashboard‚Äôs purpose** is to provide clarity, direction, and answers‚Äî**at a glance**, under pressure, in motion.

A good dashboard:
- Shows relevant data with minimal distraction
- Surfaces system health, performance, and reliability
- Highlights degradation before users complain
- Enables fast diagnosis during incidents
- Reinforces the contract between service and consumer

---

## üóÇ Dashboard Types (Know What You‚Äôre Building)

| Dashboard Type     | Purpose                                      | Audience                  |
|--------------------|----------------------------------------------|---------------------------|
| **Executive Summary** | SLIs/SLOs, high-level indicators             | VPs, PMs, TLs              |
| **Service Health**     | Core KPIs + alert-aligned metrics           | SREs, devs, on-call        |
| **Incident Triage**    | Drilldowns for debugging issues             | On-call responder         |
| **Postmortem View**    | Reconstruct outage timeline + causality     | Retrospective participants |
| **Business KPIs**      | Metrics tied to user behavior, revenue, etc | Product, stakeholders     |

> ‚ùó Most Grafana instances blend 3+ dashboard types into one spaghetti panel. Don't do this. Build with **intent**.

---

## üìê Dashboard Layout: Top-Down Clarity

> üó∫ Mermaid: Standard Dashboard Layout  
```mermaid
graph TD
A[Top Row: SLIs / Key Health Indicators]
B[Middle Row: Service-Level KPIs]
C[Bottom Row: Debugging / System Metrics]
A --> B --> C
```

- **Top Row**: p95 latency, availability %, error rate  
- **Middle Row**: request volume, retry counts, CPU/memory saturation  
- **Bottom Row**: queues, threads, GC, db connections, pod status

‚úÖ Group by use‚Äînot by ‚Äúwe had space.‚Äù

---

## üìä Every Panel Answers a Question

| Panel Title                     | Question It Answers                              |
|--------------------------------|--------------------------------------------------|
| ‚Äúp95 Checkout Latency‚Äù         | ‚ÄúAre users experiencing slowness right now?‚Äù     |
| ‚Äú5xx Rate by Service‚Äù          | ‚ÄúWhich services are failing, and how badly?‚Äù     |
| ‚ÄúOrder Volume by Region‚Äù       | ‚ÄúIs traffic normal across regions?‚Äù              |
| ‚ÄúCPU Saturation ‚Äì Checkout‚Äù    | ‚ÄúIs infra limiting app performance?‚Äù             |
| ‚ÄúError Budget Burn‚Äù            | ‚ÄúAre we exceeding our SLOs, and how fast?‚Äù       |

> üî• **If a panel doesn‚Äôt answer a specific, operationally useful question, delete it.**

---

## üé® Visual Design Principles

- ‚úÖ **Consistent color semantics**  
  - Green = healthy  
  - Yellow = degraded  
  - Red = broken  
- ‚úÖ Use **units** (ms, %, req/s). Never show raw numbers unlabeled.
- ‚úÖ Limit graph lines: 3‚Äì5 max per panel. Use legends.
- ‚úÖ Align time ranges globally across all panels.
- ‚úÖ Annotate with deploys, incidents, and alerts.

> üí° Avoid:
> - Rainbow color palettes  
> - Pie charts with > 3 slices  
> - Panels titled "Graph 1"

---

## üõ† Panel Type Cheat Sheet

| Panel Type     | Use Case                                             |
|----------------|------------------------------------------------------|
| Time Series    | Show trends over time (latency, rate, throughput)    |
| Single Stat    | Show current value (availability %, error rate)      |
| Bar Chart      | Compare across entities (region, service)            |
| Heatmap        | Visualize histograms, latency buckets                |
| Table          | Show raw values with labels                          |
| Alert List     | Integrate active alerts into dashboard               |

---

## ‚ö†Ô∏è Dashboard Anti-Patterns

| Smell                     | Why It Hurts                                 | Fix                                   |
|---------------------------|----------------------------------------------|----------------------------------------|
| Panel without units       | Ambiguity ‚Üí misinterpretation                | Always label Y-axis and stat units     |
| 15+ lines in one graph    | Visual clutter, unreadable under pressure    | Limit lines, use filters or topN       |
| All green all the time    | No context = no value                        | Use thresholds and color ranges        |
| Mixing prod + dev metrics | Confusing signals                            | Separate dashboards or use variables   |
| Using averages everywhere | Masks outliers                               | Use percentiles (p95, p99)             |

---

## üß™ Examples

### ‚úÖ Good Panel Setup

```promql
histogram_quantile(0.95,
  sum(rate(request_duration_seconds_bucket{job="checkout"}[5m]))
  by (le)
)
```
- Title: `Checkout Latency (p95)`
- Y-axis: `ms`
- Threshold: Yellow @ 500ms, Red @ 1000ms

---

### ‚ùå Bad Panel Setup

```promql
http_requests_total
```
- Title: `Requests`
- No `rate()` ‚Üí not showing activity
- No filters ‚Üí mixed HTTP status
- No group ‚Üí ambiguous source

---

## üîÅ Templating & Variables

- Use `region`, `service`, `env`, and `team` as variables
- Bind time ranges and template options globally
- Keep variable dropdowns clean and **bounded**

---

## üß± Dashboard Build Process

> üß∞ Mermaid: From Question to Panel  
```mermaid
flowchart LR
A[Operational Question] --> B[Identify Metric(s)]
B --> C[Build PromQL Query]
C --> D[Choose Panel Type]
D --> E[Test Display and Legends]
E --> F[Apply Thresholds & Titles]
F --> G[Deploy with Team Review]
```

---

## üí¨ Business/SLI Panel Examples

- ‚úÖ **‚ÄúCheckout Availability‚Äù**
```promql
(
  sum(rate(http_requests_total{status=~"2..", job="checkout"}[5m]))
/
  sum(rate(http_requests_total{job="checkout"}[5m]))
) * 100
```

- ‚úÖ **‚ÄúError Budget Burn (Fast Track)‚Äù**
```promql
(
  sum(rate(http_requests_total{status=~"5..", job="checkout"}[1m]))
/
  sum(rate(http_requests_total{job="checkout"}[1m]))
) / 0.001 > 1
```

> This panel tells your team whether you're on pace to *blow your SLO*.

---

## ‚úÖ Key Takeaways

- Dashboards are communication tools. Make them readable, focused, and operational.
- Build from top (SLIs) to bottom (infrastructure)
- Every panel should justify its existence with a question it answers
- Limit complexity; increase clarity
- Use consistent design and naming patterns

---

## üßæ Bonus: One-Pager ‚Äì PromQL Patterns Cheat Sheet

| Use Case                     | Query |
|------------------------------|--------|
| Request Rate                 | `rate(http_requests_total[5m])` |
| 5xx Rate by Service          | `sum by (service)(rate(http_requests_total{status=~"5.."}[5m]))` |
| Latency p95                  | `histogram_quantile(0.95, sum(rate(duration_bucket[5m])) by (le))` |
| Availability (Success Ratio) | `(2xx + 3xx) / all` |
| CPU Saturation               | `1 - avg by (instance)(rate(cpu_seconds_total{mode="idle"}[5m]))` |
| Queue Length Gauge           | `queue_length{job="api"}` |
| Burn Rate (SLO)              | `error_rate / error_budget_fraction` |

---

## ‚è≠Ô∏è Next Up: Section 4 ‚Äì Defining and Operationalizing SLIs  
Where we take these dashboards and connect them directly to service reliability, error budgets, and the contracts you actually need to keep.
