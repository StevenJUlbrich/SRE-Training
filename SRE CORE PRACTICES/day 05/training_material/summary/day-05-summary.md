# Dayâ€¯5Â â€” Integration &Â Triage **Grand Summary**  
*Graphicâ€‘Novel EditionÂ â€¢ Mentor: **Mei â€œThe Panic Plannerâ€Â Lin***

---

## â€¢ Opening Voiceâ€‘over  
Before the first panel appears, Mei narrates in an offâ€‘screen caption:

> *â€œReliability is a staircase: first you crawl with habits, then you walk with templates, then you run with automation. Tonight, we look down from the topâ€”so you can climb faster tomorrow.â€*

---

## PanelÂ 1Â â€” Beginner â€¢ **SpotÂ theÂ Change** <!-- {{panel:1}} -->
<table>
  <tr>
    <td style="width:50%; vertical-align:top; padding-right:16px;">
      <strong>ğŸ¯ Learning Objective</strong><br>
      Master the habit of checking recent deployments, feature flags and config pushes <em>before</em> touching an alert.<br><br>
      <strong>âœ… Takeaway</strong><br>
      More than 70â€¯% of production incidents trace back to a fresh change. Track the breadcrumb, trace the break.
    </td>
    <td style="width:50%;">
      <img src="images/summary_panel_01_beginner_integration.png" width="450" alt="Mei highlights a deploy feed" />
    </td>
  </tr>
</table>

### Narrative
Alex (rookie SRE) bursts into the warâ€‘room: â€œError rate on checkout jumped!â€  Mei raises a finger, tapping a deploy dashboard that lists _checkoutâ€‘serviceÂ v2.4.1_ shipped three minutes earlier. â€œFirst habit,â€ she says. â€œFindÂ the change; half the time, it <em>is</em> the cause.â€  She copies the version number into a sticky note titled **Rootâ€‘Cause Suspects**.

### Practical NuggetÂ â€” 60â€‘Second Deploy Audit
1. **Prometheus/Grafana**Â â€” Query `changes_last_4h{service="checkout"}`.  
2. **Datadog Events**Â â€” Filter `tags:deploy service:checkout`.  
3. **Splunk**Â â€” `index=deploy service=checkout earliest=-4h`.  
4. **Flag Service**Â â€” `flagctl list --service checkout`.

*If no change appears, your suspect list moves to dependencies; but 7Â times in 10, this tiny audit saves twenty minutes of blind logâ€‘grepping.*

---

## PanelÂ 2Â â€” Beginner â€¢ **TriageÂ LikeÂ Clockwork** <!-- {{panel:2}} -->
<table>
  <tr>
    <td style="width:50%; vertical-align:top; padding-right:16px;">
      <strong>ğŸ¯ Learning Objective</strong><br>
      Apply the 5â€‘Question Drill in under 90Â seconds to every new alert.<br><br>
      <strong>âœ… Takeaway</strong><br>
      SymptomÂ â†’â€¯ScopeÂ â†’â€¯TimestampÂ â†’â€¯ChangeÂ â†’â€¯Owner. Nail those and panic fades.
    </td>
    <td style="width:50%;">
      <img src="images/summary_panel_02_beginner_triage.png" width="450" alt="Alex fills 5-question drill" />
    </td>
  </tr>
</table>

### Narrative
Latâ€‘ency alarms flash red. Mei slides a laminated card to Alex:
```text
WHAT?  WHEN?  SCOPE?  RECENT CHANGE?  OWNER?
```
Together they fill it:
* **WHAT:** `checkout_latency_p95` breached 2Â s.  
* **WHEN:** Started 21:06Â UTC, now 4â€¯m long.  
* **SCOPE:** 18â€¯% of regional traffic.  
* **RECENT CHANGE:** v2.4.1 at 21:02.  
* **OWNER:** teamâ€‘checkout (PagerDuty).

Alex exhalesâ€”first wave of uncertainty gone.

### Practical NuggetÂ â€” Triage Clipboard Template
Keep a clipboard (physical or Notion) with the drill, plus empty fields for **IncidentÂ #**, **SlackÂ Channel**, **TicketÂ ID**. Filling them forces structure when adrenaline spikes.

---

## PanelÂ 3Â â€” Intermediate â€¢ **DashboardsÂ asÂ Code** <!-- {{panel:3}} -->
<table>
  <tr>
    <td style="width:50%; vertical-align:top; padding-right:16px;">
      <strong>ğŸ¯ Learning Objective</strong><br>
      Convert adâ€‘hoc Grafana boards into Jsonnet templates rendered by CI.<br><br>
      <strong>âœ… Takeaway</strong><br>
      A dashboard committed to Git gets linted, diffed, and rolled back like any other deploy.
    </td>
    <td style="width:50%;">
      <img src="images/summary_panel_03_intermediate_dash.png" width="450" alt="PR diff shows Jsonnet dashboard" />
    </td>
  </tr>
</table>

### Narrative
JC merges **PRÂ #482** adding `dashboards/checkout/red.jsonnet`. CI job **`render-dashboards.yml`** runs `jsonnet-lint`, renders JSON, diffs against Grafana, and posts a screenshot to **#dashâ€‘updates**. Alex sees that the new board already carries deploy annotationsâ€”no more midnight pixelâ€‘drift.

### Practical NuggetÂ â€” Minimal Jsonnet Pattern
```jsonnet
local svc = std.extVar('service');
local p(t,e) = {title:t,type:'graph',targets:[{expr:e}]};
{
  title: svc + ' RED',
  tags: ['red', svc],
  rows: [{ panels: [
    p('Rate','sum(rate(http_requests_total{service="'+svc+'"}[5m]))'),
    p('Errors','sum(rate(http_requests_total{service="'+svc+'",code=~"5.."}[5m]))'),
    p('Duration p95','histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{service="'+svc+'"}[5m])) by (le))')
  ]}]
}
```
Commit once; parametrize for every service.

---

## PanelÂ 4Â â€” Intermediate â€¢ **SmartÂ Routing, SilentÂ Noise** <!-- {{panel:4}} -->
<table>
  <tr>
    <td style="width:50%; vertical-align:top; padding-right:16px;">
      <strong>ğŸ¯ Learning Objective</strong><br>
      Group and deduplicate alerts, then route by severity &Â owner labels.<br><br>
      <strong>âœ… Takeaway</strong><br>
      One grouped page beats thirty duplicate pingsâ€”engineers stay fresh, MTTR shrinks.
    </td>
    <td style="width:50%;">
      <img src="images/summary_panel_04_intermediate_routing.png" width="450" alt="Alertmanager dedup diagram" />
    </td>
  </tr>
</table>

### Narrative
During a minor spike, Alertmanager receives 30Â identical payloads. Routing rules group by `service+impact`, hold 30Â s, dedup 5Â min, and page **teamâ€‘billing** <em>once</em>. Slack thread shows just one noiseâ€‘free incident card.

### Practical NuggetÂ â€” Routing Snippet
```yaml
route:
  group_by: [service, impact]
  group_wait: 30s
  group_interval: 5m
  receiver: pagerduty
```
Add silence windows via `--maintenance` label so deployâ€‘time flaps donâ€™t page.

---

## PanelÂ 5Â â€” SRE â€¢ **AlertsÂ ThatÂ FixÂ Themselves** <!-- {{panel:5}} -->
<table>
  <tr>
    <td style="width:50%; vertical-align:top; padding-right:16px;">
      <strong>ğŸ¯ Learning Objective</strong><br>
      Attach webhooks or Lambdas that roll back feature flags or restart pods within 60Â seconds of a P2/P3 alert.<br><br>
      <strong>âœ… Takeaway</strong><br>
      Zeroâ€‘page incidents = true selfâ€‘healing success.
    </td>
    <td style="width:50%;">
      <img src="images/summary_panel_05_sre_selfheal.png" width="450" alt="Webhook rolls back flag" />
    </td>
  </tr>
</table>

### Narrative
A `billing_errors_total` spike triggers PagerDuty Event Orchestration. Webhook `https://fn.toggle/api/v1/flags/billing-calc/off` fires, flag rolls back, error rate drops in 20Â s. A metric `selfheal_success_total{service="billing"}` increments, and no human was paged.

### Practical NuggetÂ â€” Selfâ€‘Healing Metrics
* `selfheal_attempt_total{service}`  
* `selfheal_success_total{service}`  
* Alert if `rate(success_total)/rate(attempt_total) < 0.8` over 10Â m.

---

## PanelÂ 6Â â€” SRE â€¢ **ProveÂ ItÂ withÂ Chaos** <!-- {{panel:6}} -->
<table>
  <tr>
    <td style="width:50%; vertical-align:top; padding-right:16px;">
      <strong>ğŸ¯ Learning Objective</strong><br>
      Run latencyâ€‘injection experiments to validate SLOs and selfâ€‘healing under failure.<br><br>
      <strong>âœ… Takeaway</strong><br>
      Controlled chaos beats surprise chaos.
    </td>
    <td style="width:50%;">
      <img src="images/summary_panel_06_sre_chaos.png" width="450" alt="Chaos injection badge" />
    </td>
  </tr>
</table>

### Narrative
Nightly nonâ€‘prod chaos job injects 300Â ms latency to PaymentSvc for 5Â % of canary users. If `checkout_latency_p95` breaches 1Â s for over 90Â s, the experiment autoâ€‘halts. Dashboard lights green: **Chaos Passed**â€”selfâ€‘healing flag rollback handled the slowdown.

### Practical NuggetÂ â€” Safety Switch Command
`/gremlin halt checkout` in Slack stops any running experiment; only `sre-primary` role can invoke it.

---

## PanelÂ 7Â â€” SRE â€¢ **MeasureÂ Capacity, AvoidÂ Surprises** <!-- {{panel:7}} -->
<table>
  <tr>
    <td style="width:50%; vertical-align:top; padding-right:16px;">
      <strong>ğŸ¯ Learning Objective</strong><br>
      Run monthly spike &Â soak tests via k6 and graph capacity trends.<br><br>
      <strong>âœ… Takeaway</strong><br>
      Autoscalers fail quietlyâ€”capacity graphs catch them early.
    </td>
    <td style="width:50%;">
      <img src="images/summary_panel_07_sre_capacity.png" width="450" alt="Capacity trend graph" />
    </td>
  </tr>
</table>

### Narrative
CI job **`capacity-trend.yml`** ramps to 1â€¯000Â RPS, sustains 8Â minutes, and plots `rps_vs_cpu` in Grafana. Trendline shows capacity slipped 6â€¯% this month: GC pauses longer. An action item enters next sprintâ€”tune JVM heap.

### Practical NuggetÂ â€” Loadâ€‘Test Types Quick Ref
* **Spike:** sudden 5Ã— trafficâ€”tests autoscaler speed.  
* **Stress:** grow until failureâ€”find absolute max.  
* **Soak:** 12â€¯h steadyâ€”exposes memory leaks.

---

## PanelÂ 8Â â€” **Reflection Loop &Â CallÂ toÂ Action** <!-- {{panel:8}} -->
<table>
  <tr>
    <td style="width:50%; vertical-align:top; padding-right:16px;">
      <strong>ğŸ¯ Learning Objective</strong><br>
      See the staircase: Habits â†’ Templates â†’ Automation â†’ Validation.<br><br>
      <strong>âœ… Takeaway</strong><br>
      â€œTrack, template, test, and teachâ€”continuous SRE growth.â€<br><br>
      **Next Steps**  
      1. Pick one noisy alert âœ add `service` &Â `owner` labels.  
      2. Convert one freestyle dashboard âœ Jsonnet.  
      3. Draft a selfâ€‘healing webhook âœ count success rates.  
      4. Define a chaos hypothesis tied to an SLO.
    </td>
    <td style="width:50%;">
      <img src="images/summary_panel_08_reflection.png" width="450" alt="Final reflection panel" />
    </td>
  </tr>
</table>

---

## Epilogue Caption  
> *Mei:* â€œToday you practiced watchfulness, codified vigilance, and automated calm. Keep climbing.â€

---

*End of Dayâ€¯5 Graphicâ€‘Novel Summary*

