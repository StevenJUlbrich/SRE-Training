# **DayÂ 10 AnswerÂ Sheet**

## AnswerÂ 1: Multiâ€‘Database Incident Basics  
ğŸ” Beginnerâ€‘Level | Multiple Choice  

**Question:**  
Rafael likens a polyglot outage to a â€œcircus where every performer juggles a different fireÂ torch.â€ Which challenge does this analogy highlight?  

A. All databases always share the same backup window  
B. Coordinating different failure modes and data timeliness across databases  
C. Redis always loses data before Oracle in an outage  
D. Every database can be monitored with a single exporter  

**CorrectÂ Answer:** **B**

**Explanation:**  
The analogy underscores the chaos of managing divergent failure patterns, replication lags, and recovery timelines across heterogeneous databases during the same incident.

**Why other options are incorrect:**  
- **A:** Backup windows may differ, but that is not the core â€œcircusâ€ problem.  
- **C:** Redis is not guaranteed to fail first; outage impact varies by workload.  
- **D:** A single exporter cannot surface deep techâ€‘specific metrics.  

**Database ComparisonÂ Note:** Oracle may stall on rowâ€‘locks, while Cassandra degrades with tombstones; synchronising such mismatched behaviours is the real juggling act.  

**Knowledge Connection:** Mirrors Rafaelâ€™s opening story where Oracle, Cassandra, and Kafka each â€œdroppedâ€ data differently.  

**SRE Perspective:** Build composite runbooks so onâ€‘call engineers diagnose crossâ€‘DB issues holistically, not sequentially.  

**AdditionalÂ Insight:** Adopt an eventâ€‘bus (Kafka/CDC) to provide a consistent replay mechanism when one juggler drops the torch.  

**VisualÂ Explanation:** *Not required*  

---

## AnswerÂ 2: Sourceâ€‘ofâ€‘Truth Identification  
ğŸ” Beginnerâ€‘Level | Multiple Choice  

**Question:**  
During the DayÂ 10 opening incident, which system had to be trusted first when order data diverged?  

A. Cassandra catalog  
B. Kafka replay topic  
C. Oracle OLTP database  
D. Redis cache  

**CorrectÂ Answer:** **C**

**Explanation:**  
Oracle held the transactional record of orders with ACID guarantees, making it the authoritative source to reconcile other stores.

**Why other options are incorrect:**  
- **A:** The catalog lacked completeness for orders.  
- **B:** Kafka could replay but still relied on Oracleâ€‘originated events.  
- **D:** Cache may contain stale entries.  

**Database ComparisonÂ Note:** In hybrid stacks, an ACID RDBMS often functions as systemâ€‘ofâ€‘record; eventualâ€‘consistent stores mirror it asynchronously.  

**Knowledge Connection:** Matches Rafaelâ€™s phrase â€œOracle is my ledger of truth; everyone else syncs to it.â€  

**SRE Perspective:** Always document which datastore is authoritative per domain to shorten RCA time.  

**AdditionalÂ Insight:** Store the authoritative DB identifier in event payloads to enable idempotent downstream updates.  

**VisualÂ Explanation:** *Not required*  

---

## AnswerÂ 3: Eventâ€‘Driven Bridge  
ğŸ” Beginnerâ€‘Level | Fillâ€‘inâ€‘theâ€‘Blank  

**Question:**  
In Rafaelâ€™s hybrid stack, **Kafka** acts primarily as the ________ between relational and NoSQL databases, decoupling write paths and enabling replay.  

A. primary OLTP store  
B. integration bus  
C. archival backup tier  
D. schema registry  

**CorrectÂ Answer:** **BÂ â€“Â integration bus**

**Explanation:**  
Kafka buffers change events, letting producers commit independently while consumers replay at their own pace, thereby bridging SQL and NoSQL worlds.

**Why other options are incorrect:**  
- **A:** Oracle, not Kafka, is the OLTP store.  
- **C:** Kafka provides retention but is not a longâ€‘term backup platform.  
- **D:** Confluent Schema Registry is a separate component.  

**Database ComparisonÂ Note:** Debezium for Oracle CDC and Kafka Connect for DynamoDB both rely on Kafka as the transport layer.  

**Knowledge Connection:** Reflects Rafaelâ€™s â€œKafka = polyglot glueâ€ mantra.  

**SRE Perspective:** Monitor consumer lag to ensure this â€œglueâ€ doesnâ€™t quietly unstick.  

**AdditionalÂ Insight:** Partition topics by business key to simplify targeted replays.  

**VisualÂ Explanation:** *Not required*  

---

## AnswerÂ 4: Backup Drill Schedule  
ğŸ” Beginnerâ€‘Level | Multiple Choice  

**Question:**  
Rafaelâ€™s **Polyglot Commandments** demand running restore drills at least how often?  

A. Monthly  
B. Quarterly  
C. Biâ€‘annually  
D. Only after an incident  

**CorrectÂ Answer:** **B**

**Explanation:**  
Quarterly restore drills validate that backups remain usable and that operators retain muscle memory, striking a balance between diligence and effort.

**Why other options are incorrect:**  
- **A:** Monthly is good but not explicitly required.  
- **C:** Sixâ€‘month gaps risk stale runbooks.  
- **D:** Waiting for incidents defeats proactive resilience.  

**Database ComparisonÂ Note:** For DynamoDB, tableâ€‘level restores must also be exercised quarterly to verify IAM and pointâ€‘inâ€‘time settings.  

**Knowledge Connection:** Direct from CommandmentÂ #6: â€œEvery DB must have a **tested** backup & restore.â€  

**SRE Perspective:** Schedule restore drills as changeâ€‘controlled activities to avoid production clashes.  

**AdditionalÂ Insight:** Alternate fullâ€‘cluster and targetedâ€‘table restores to cover both extremes.  

**VisualÂ Explanation:** *Not required*  

---

## AnswerÂ 5: Monitoring Dashboards  
ğŸ” Beginnerâ€‘Level | True/False  

**Question:**  
True or False: Rafael insists on separate Grafana dashboards for each database to avoid clutter.  

A. True  
B. False  

**CorrectÂ Answer:** **BÂ â€“Â False**

**Explanation:**  
Rafael advocates a **single consolidated dashboard** so correlations across Oracle, Cassandra, and Kafka appear sideâ€‘byâ€‘side during incidents.

**Database ComparisonÂ Note:** Shared panels let teams overlay Oracle wait events with Kafka lag, exposing causal chains.  

**Knowledge Connection:** Rafaelâ€™s tip: â€œSeparate dashboards mean separate pagers.â€  

**SRE Perspective:** Consolidated views reduce MTTR and paging warâ€‘rooms.  

**AdditionalÂ Insight:** Use templating variables in Grafana to switch databases within one panel instead of duplicating.  

**VisualÂ Explanation:** *Not required*  

---

## AnswerÂ 6: Scaling Strategies  
ğŸ” Beginnerâ€‘Level | Matching  

**Question:**  
Match each technology with its *default* scaling model.  

ColumnÂ A â†’ ColumnÂ B  
1. OracleÂ  â€ƒâ€ƒâ†’ **C**  
2. Cassandra â†’ **B**  
3. DynamoDB â†’ **A**  

**Correct Matches:**  
1Â â€“Â Câ€ƒPrimarily vertical plus partitioning  
2Â â€“Â Bâ€ƒHorizontal node expansion  
3Â â€“Â Aâ€ƒAutoâ€‘scaling throughput tiers  

**Explanation:**  
Oracle traditionally scales by adding CPU/RAM or sharding; Cassandra spreads data across more nodes; DynamoDB autoscales provisioned or onâ€‘demand capacity.  

**Database ComparisonÂ Note:** Horizontal scaling in Cassandra demands consistent hashing; Oracle sharding requires global services catalog.  

**Knowledge Connection:** Covered in Rafaelâ€™s â€œscale knobsâ€ table.  

**SRE Perspective:** Factor scaling model into capacityâ€‘planning budgetsâ€”node count vs. license cost.  

**AdditionalÂ Insight:** Mix models: keep Oracle small but feed writes through horizontally scalable Kafka to absorb bursts.  

**VisualÂ Explanation:** *Not required*  

---

## AnswerÂ 7: Consumer Lag Threshold  
ğŸ” Beginnerâ€‘Level | Ordering  

**Question:**  
Arrange Rafaelâ€™s lag triage steps in order:  

A. Restart or scale stuck consumer  
B. Inspect Kafka consumer group offsets  
C. Verify Oracle commit success  
D. Notify onâ€‘call channel of lag root cause  

**Correct Order:** **B,Â C,Â A,Â D**

**Explanation:**  
First confirm lag (B), then ensure the source commits exist (C). If data is present, remediate the consumer (A). Finally, communicate findings (D).

**Database ComparisonÂ Note:** Postgres logical replication follows similar diagnoseâ€‘sourceâ€‘thenâ€‘consumer flow.  

**Knowledge Connection:** Aligns with Rafaelâ€™s incident playbook flowchart.  

**SRE Perspective:** Broadcasting after remediation avoids alert noise and focuses discussion on actionables.  

**AdditionalÂ Insight:** Automate a chatbot to post offset graphs when stepÂ B detects â‰¥Â 10Â K backlog.  

**VisualÂ Explanation:**  
```mermaid
flowchart LR
  B1["Detect Lag"] --> C1["Verify Source Commits"]
  C1 --> A1["Restart/Scale Consumer"]
  A1 --> D1["Notify Onâ€‘Call"]
```  

---

## AnswerÂ 8: Hybrid Architecture Diagram  
ğŸ§© Intermediateâ€‘Level | Diagramâ€‘Based  

**Question:**  
```mermaid
flowchart LR
    A["App Services"] -->|Writes| O["Oracle"]
    A -->|Reads/Writes| C["Cassandra"]
    O -->|CDC Events| K["Kafka"]
    K -->|Backfill| D["DynamoDB"]
```  

According to Rafael, which component **decouples** Oracle from NoSQL backâ€‘ends in this flow?  

A. App Services  
B. Oracle  
C. Kafka  
D. DynamoDB  

**CorrectÂ Answer:** **C**

**Explanation:**  
Kafka ingests Oracle changeâ€‘dataâ€‘capture events, buffering them for asynchronous consumption by DynamoDB (and others), thereby isolating transactional writes from NoSQL ingestion speed.

**Why other options are incorrect:**  
- **A:** App services still couple writes directly to Oracle.  
- **B:** Oracle is the producer, not the decoupler.  
- **D:** DynamoDB is a downstream consumer.  

**Database ComparisonÂ Note:** Similar CDC pipelines use Debezium + Kafka for MySQL, or GoldenGate for Oracle.  

**Knowledge Connection:** Matches Rafaelâ€™s â€œKafka as shock absorberâ€ metaphor.  

**SRE Perspective:** Track *endâ€‘toâ€‘end lag* between Oracle SCN and DynamoDB timestamp to validate the decoupling still meets SLA.  

**AdditionalÂ Insight:** Attach a schema registry to the CDC topic to version payloads.  

**VisualÂ Explanation:** *Original diagram retained above*  

---

## AnswerÂ 9: Stuck Offset War Story  
ğŸ§© Intermediateâ€‘Level | Multiple Choice  

**Question:**  
In Rafaelâ€™s â€œKafka consumer freezeâ€ incident, what primary metric revealed the problem?  

A. Oracle buffer cache hit ratio  
B. Cassandra tombstone count  
C. Kafka consumer lag  
D. DynamoDB provisioned throughput  

**CorrectÂ Answer:** **C**

**Explanation:**  
A plateauing consumerâ€‘lag graph exposed that the consumer offset stopped advancing despite incoming messages, pointing to a stalled worker.

**Why other options are incorrect:**  
- **A,Â B,Â D:** These metrics did not surface the streaming bottleneck.  

**Database ComparisonÂ Note:** Similar lag alarms exist for Kinesis (IteratorAge) or Pulsar (Backlog).  

**Knowledge Connection:** Echoes Rafaelâ€™s CommandmentÂ #1 â€œIf it streams, it breaks silently.â€  

**SRE Perspective:** Alert on lag derivative (rate of change) to catch freezes faster than absolute thresholds.  

**AdditionalÂ Insight:** Implement a liveness probe on consumers to autoâ€‘restart if no offset progress in N minutes.  

**VisualÂ Explanation:** *Not required*  

---

## AnswerÂ 10: Backup Methods Comparison  
ğŸ§© Intermediateâ€‘Level | Multiple Choice  

**Question:**  
Which pairing correctly matches *technology â†’ recommended backup approach*?  

A. Oracle â†’ nodetool snapshot  
B. Cassandra â†’ RMAN  
C. Kafka â†’ crossâ€‘broker topic replication  
D. DynamoDB â†’ physical filesystem copy  

**CorrectÂ Answer:** **C**

**Explanation:**  
Kafkaâ€™s durability relies on multiâ€‘broker replication; snapshots like nodetool or RMAN are irrelevant.  

**Why other options are incorrect:**  
- **A:** nodetool is Cassandraâ€™s snapshot utility.  
- **B:** RMAN is Oracleâ€™s native backup tool.  
- **D:** DynamoDB is managed; filesystem copies are impossible.  

**Database ComparisonÂ Note:** In Kafka, mirrorâ€‘maker or cluster replication ensures topic redundancy akin to RAID for logs.  

**Knowledge Connection:** Referenced in Rafaelâ€™s backup matrix.  

**SRE Perspective:** Retain at least three replicas and verify ISR (inâ€‘sync replica) count daily.  

**AdditionalÂ Insight:** Use clusterâ€‘toâ€‘cluster replication for geoâ€‘redundancy.  

**VisualÂ Explanation:** *Not required*  

---

## AnswerÂ 11: Polyglot Commandment Focus  
ğŸ§© Intermediateâ€‘Level | True/False  

**Question:**  
True or False: One of Rafaelâ€™s commandments states **â€œIf it streams, it breaks silentlyâ€”monitor the lag.â€**  

A. True  
B. False  

**CorrectÂ Answer:** **AÂ â€“Â True**

**Explanation:**  
Lag is a silent symptom of consumer stalls; monitoring it reveals hidden replication backlog before data loss surfaces.

**Database ComparisonÂ Note:** Similar concept in CDC pipelines across Debezium, Snowflake Snowpipe, etc.  

**Knowledge Connection:** Directly quoted from CommandmentÂ #1.  

**SRE Perspective:** Include lag SLIs in errorâ€‘budget calculations.  

**AdditionalÂ Insight:** Correlate lag spikes with partitionâ€‘rebalance events to identify root causes.  

**VisualÂ Explanation:** *Not required*  

---

## AnswerÂ 12: Role Summary Grid  
ğŸ§© Intermediateâ€‘Level | Fillâ€‘inâ€‘theâ€‘Blank  

**Question:**  
Rafaelâ€™s grid lists **Redis** as ideal for ________ because of inâ€‘memory speed and advanced data structures.  

A. ACID ledger entries  
B. longâ€‘term analytics storage  
C. caching and ephemeral data  
D. multiâ€‘document joins  

**CorrectÂ Answer:** **CÂ â€“Â caching and ephemeral data**

**Explanation:**  
Redis excels at microsecond reads/writes and TTL keys, perfect for session or leaderboard caching.

**Why other options are incorrect:**  
- **A:** Redis lacks durable ACID semantics.  
- **B:** RAM is too costly for long retention.  
- **D:** Redis offers hashes/sets but not rich join semantics.  

**Database ComparisonÂ Note:** Memcached provides similar caching but fewer data structures.  

**Knowledge Connection:** Seen in Rafaelâ€™s grid line â€œRedis: speed, advanced structures, memoryâ€‘bound.â€  

**SRE Perspective:** Watch eviction rate metrics to avoid surprise data loss.  

**AdditionalÂ Insight:** Enable Redis persistence (AOF/RDB) only when you need warmâ€‘cache survivability.  

**VisualÂ Explanation:** *Not required*  

---

## AnswerÂ 13: Productionâ€‘Readiness Flow  
ğŸ§© Intermediateâ€‘Level | Multiple Choice  

**Question:**  
According to Rafaelâ€™s readiness flowchart, which verification **must** occur immediately after confirming a *quarterly restore drill* is complete?  

A. Monitoring & observability in place  
B. Demo of failover DNS switchâ€‘over  
C. Disasterâ€‘recovery budget approval  
D. Developer signâ€‘off on schema design  

**CorrectÂ Answer:** **A**

**Explanation:**  
The flowchart proceeds from restoreâ€‘drill success to ensuring observability before granting production signâ€‘off.

**Why other options are incorrect:**  
- **B,Â C,Â D:** Valuable, but appear later or outside the depicted flow.  

**Database ComparisonÂ Note:** Observability must cover both backup and restore metrics (snapshot age, restore duration).  

**Knowledge Connection:** Matches the Mermaid flowchart in sectionÂ 7.  

**SRE Perspective:** An unmonitored backup is indistinguishable from no backup.  

**AdditionalÂ Insight:** Automate alerts on backup age exceeding RPO.  

**VisualÂ Explanation:** *Supplementary readiness miniâ€‘flow*  
```mermaid
flowchart LR
  R["Restore Drill âœ“"] --> M["Monitoring?"] --> P["Prod Ready"]
```  

---

## AnswerÂ 14: Monitoring Stack Diagram  
ğŸ’¡ Advanced/SREâ€‘Level | Diagramâ€‘Based  

**Question:**  
```mermaid
flowchart TB
  subgraph "Monitoring Stack"
    Prom["Prometheus Exporters"]
    Dash["Grafana"]
  end
  DB1["Oracle"] --> Prom
  DB2["Kafka"] --> Prom
  Prom --> Dash
```  

Which SRE metric is **MOST** critical for Kafka in this diagram when ensuring crossâ€‘database consistency?  

A. Logâ€‘compaction byte rate  
B. Consumer group lag  
C. Oracle wait events  
D. Grafana dashboard refresh rate  

**CorrectÂ Answer:** **B**

**Explanation:**  
Consumer lag directly indicates whether Kafka data is caught up with producers like Oracle, impacting downstream consistency.

**Why other options are incorrect:**  
- **A:** Important for retention, but not immediate consistency.  
- **C:** Oracle metric, not Kafka.  
- **D:** Dashboard refresh rate doesnâ€™t affect data flow.  

**Database ComparisonÂ Note:** In Pulsar, equivalent metric is â€œsubscription backlog.â€  

**Knowledge Connection:** Rafaelâ€™s alert rule: â€œLag >Â 10Â K triggers red.â€  

**SRE Perspective:** Combine lag with 99thâ€‘percent latency SLI to reflect user impact.  

**AdditionalÂ Insight:** Alert on *stalled lag* (zero progress) faster than on size alone.  

**VisualÂ Explanation:** *Original diagram retained above*  

---

## AnswerÂ 15: Failure Mode Analysis  
ğŸ’¡ Advanced/SREâ€‘Level | Multiple Choice  

**Question:**  
Rafael highlights **Cassandra tombstone overload** as a common failure. Which symptom typically precedes a tombstoneâ€‘related crash?  

A. Rapid increase in Oracle redo log size  
B. Dramatic drop in read latency due to key cache hits  
C. Spike in read latency accompanied by â€œtombstone_warn_thresholdâ€ alerts  
D. Kafka broker diskâ€‘full error  

**CorrectÂ Answer:** **C**

**Explanation:**  
Excess tombstones force Cassandra to scan deleted rows, ballooning read latency and triggering threshold warnings.

**Why other options are incorrect:**  
- **A,Â D:** Unrelated to Cassandra.  
- **B:** Latency rises, not drops.  

**Database ComparisonÂ Note:** MongoDB faces similar issues with orphaned documents and fragmented indexes.  

**Knowledge Connection:** Pointed out in Rafaelâ€™s â€œtombstone Timeâ€‘bombâ€ war story.  

**SRE Perspective:** Set guardrails on deleteâ€‘heavy workloads or TTL tables.  

**AdditionalÂ Insight:** Periodic `nodetool cleanup` removes expired tombstones sooner.  

**VisualÂ Explanation:** *Not required*  

---

## AnswerÂ 16: Unified Alerting Commandment  
ğŸ’¡ Advanced/SREâ€‘Level | Fillâ€‘inâ€‘theâ€‘Blank  

**Question:**  
Rafael warns that separate alert pipelines can delay root cause identification. Therefore, he mandates routing **all critical alerts** to ________.  

A. individual DBâ€‘owner email lists  
B. a single onâ€‘call channel (e.g., Slack #sreâ€‘alert)  
C. separate Grafana panels  
D. SMS only  

**CorrectÂ Answer:** **BÂ â€“Â a single onâ€‘call channel**

**Explanation:**  
Centralised alerting ensures crossâ€‘team visibility and faster correlation between disparate database alarms.

**Why other options are incorrect:**  
- **A,Â C,Â D:** Fragment the signal or limit context.  

**Database ComparisonÂ Note:** PagerDuty or OpsGenie often integrate that unified channel.  

**Knowledge Connection:** CommandmentÂ #9 â€œObserve everything in one stream.â€  

**SRE Perspective:** Shared context slashes â€œalert pingâ€‘pong.â€  

**AdditionalÂ Insight:** Tag alerts with `db=oracle` etc. to retain specificity while sharing the pipe.  

**VisualÂ Explanation:** *Not required*  

---

## AnswerÂ 17: Incident Sequence Diagram  
ğŸ’¡ Advanced/SREâ€‘Level | Diagramâ€‘Based  

**Question:**  
```mermaid
sequenceDiagram
    participant Rafael
    participant OracleDB as Oracle
    participant KafkaSys as Kafka
    participant Dyn as DynamoDB

    Rafael->>OracleDB: Verify commit
    OracleDB-->>Rafael: Commit OK
    Rafael->>KafkaSys: Check consumer group lag
    KafkaSys-->>Rafael: Lag = 50Â K
    Rafael->>Dyn: Query new orders
    Dyn-->>Rafael: Orders missing
```  

Based on this sequence, what is Rafaelâ€™s **next** recommended action?  

A. Roll back Oracle commits  
B. Increase DynamoDB write capacity  
C. Restart or scale the stuck Kafka consumer  
D. Disable Oracle archive logging  

**CorrectÂ Answer:** **C**

**Explanation:**  
Oracle commits succeeded; lag shows Kafka consumers arenâ€™t processing events into DynamoDB. Restarting/scaling consumers unblocks ingestion.

**Why other options are incorrect:**  
- **A:** Oracle is correct.  
- **B:** Writeâ€‘capacity isnâ€™t yet the bottleneck.  
- **D:** Logging change doesnâ€™t affect lag.  

**Database ComparisonÂ Note:** In Kinesis, similar remedy is to increase shardâ€‘consumer count.  

**Knowledge Connection:** Replicates Rafaelâ€™s incident sequence in sectionÂ 8.  

**SRE Perspective:** Automate consumer selfâ€‘healing on offsetâ€‘stalled alerts.  

**AdditionalÂ Insight:** Record exact lag offset before restart to verify catchâ€‘up.  

**EnhancedÂ VisualÂ Explanation:** *Original diagram retained; optional extra omitted*  

---

## AnswerÂ 18: Multiâ€‘DB Monitoring Match  
ğŸ’¡ Advanced/SREâ€‘Level | Matching  

**Question:**  
Match each technology with the **key SRE metric** Rafael tracks.  

**Correct Matches:**  
1. OracleÂ â€“Â **C** Row lock wait events  
2. KafkaÂ Â â€“Â **A** Consumer lag (messages)  
3. CassandraÂ â€“Â **B** Read/write timeout rate  

**Explanation:**  
Rowâ€‘lock waits signal Oracle contention; consumer lag indicates Kafka pipeline health; timeouts expose Cassandra consistency issues.

**Database ComparisonÂ Note:** For DynamoDB, the cognate metric is `ThrottledRequests`.  

**Knowledge Connection:** Derived from Rafaelâ€™s Grafana examples.  

**SRE Perspective:** Surface these metrics in a single dashboard for quick drillâ€‘downs.  

**AdditionalÂ Insight:** Combine metrics into a redâ€‘amberâ€‘green (RAG) heatmap for NOC use.  

**VisualÂ Explanation:** *Not required*  

---

## AnswerÂ 19: Restoreâ€‘Drill Ordering  
ğŸ’¡ Advanced/SREâ€‘Level | Ordering  

**Question:**  
Arrange the following **restoreâ€‘drill** steps in correct order:  

A. Validate data consistency postâ€‘restore  
B. Perform simulated production restore in staging  
C. Capture and archive backup artifacts  
D. Document findings and update runbook  

**Correct Order:** **C,Â B,Â A,Â D**

**Explanation:**  
First secure backup artifacts (C), then execute the staged restore (B). Validate correctness (A) and finally document lessons (D).

**Database ComparisonÂ Note:** Oracle RMAN restores include checksum validation (stepÂ A) before archive logs are deleted.  

**Knowledge Connection:** Matches Rafaelâ€™s quarterly drill procedure.  

**SRE Perspective:** Include timing metrics during stepÂ B to benchmark RTO.  

**AdditionalÂ Insight:** Inject random corruption into backup set to test stepÂ A rigorously.  

**VisualÂ Explanation:** *Not required*  

---

## AnswerÂ 20: Polyglot Decision Matrix  
ğŸ’¡ Advanced/SREâ€‘Level | Multiple Choice  

**Question:**  
Multiple databases offer different strengths. Which option **best** represents Rafaelâ€™s **decision matrix** priority for *regulatoryâ€‘grade financial data*?  

A. Horizontal scalability over ACID transactions  
B. Low ops overhead over strict consistency  
C. Strong consistency & backup maturity over raw throughput  
D. Flexible schema over predictable latency  

**CorrectÂ Answer:** **C**

**Explanation:**  
Financial systems demand auditâ€‘grade durability and recoverability; throughput can be optimised later but correctness is mandatory.

**Why other options are incorrect:**  
- **A,Â B,Â D:** Sacrifice critical compliance requirements.  

**Database ComparisonÂ Note:** Oracle or CockroachDB often win such evaluations; Cassandra may not unless tuned for quorum writes and backups.  

**Knowledge Connection:** Rafaelâ€™s â€œRegâ€‘grade â†’ pick the safest ledger firstâ€ advice.  

**SRE Perspective:** Enforce quarterly restore drills and dualâ€‘site replication to satisfy regulators.  

**AdditionalÂ Insight:** Use appendâ€‘only tables or blockchainâ€‘style hashing to add tamperâ€‘evidence to financial ledgers.  

**VisualÂ Explanation:**  
```mermaid
flowchart LR
  Consistency["Strong Consistency"] --> Decision["Finance DB Choice"]
  Backup["Proven Backup"] --> Decision
  Throughput["Raw Throughput"] -.-> Decision
```  

---

**End of DayÂ 10 AnswerÂ Sheet**